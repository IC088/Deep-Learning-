<!DOCTYPE html>

<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="mobile-web-app-capable" content="yes">
    <title>
        Deep Learning Image Captioning Project Documentation - HackMD
    </title>
    <link rel="icon" type="image/png" href="https://hackmd.io/favicon.png">
    <link rel="apple-touch-icon" href="https://hackmd.io/apple-touch-icon.png">

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha256-916EbMg70RQy9LHiGkXzG8hSg9EdNy97GazNG/aiY1w=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/ionicons/2.0.1/css/ionicons.min.css" integrity="sha256-3iu9jgsy9TpTwXKb7bNQzqWekRX7pPK+2OLj3R922fo=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/octicons/3.5.0/octicons.min.css" integrity="sha256-QiWfLIsCT02Sdwkogf6YMiQlj4NE84MKkzEMkZnMGdg=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.1/themes/prism.min.css" integrity="sha256-vtR0hSWRc3Tb26iuN2oZHt3KRUomwTufNIf5/4oeCyg=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@hackmd/emojify.js@2.1.0/dist/css/basic/emojify.min.css" integrity="sha256-UOrvMOsSDSrW6szVLe8ZDZezBxh5IoIfgTwdNDgTjiU=" crossorigin="anonymous" />
    <style>
        @charset "UTF-8";@import url(https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,500,500i|Source+Code+Pro:300,400,500|Source+Sans+Pro:300,300i,400,400i,600,600i|Source+Serif+Pro&subset=latin-ext);.hljs{display:block;background:#fff;padding:.5em;color:#333;overflow-x:auto}.hljs-comment,.hljs-meta{color:#969896}.hljs-emphasis,.hljs-quote,.hljs-string,.hljs-strong,.hljs-template-variable,.hljs-variable{color:#df5000}.hljs-keyword,.hljs-selector-tag,.hljs-type{color:#a71d5d}.hljs-attribute,.hljs-bullet,.hljs-literal,.hljs-number,.hljs-symbol{color:#0086b3}.hljs-built_in,.hljs-builtin-name{color:#005cc5}.hljs-name,.hljs-section{color:#63a35c}.hljs-tag{color:#333}.hljs-attr,.hljs-selector-attr,.hljs-selector-class,.hljs-selector-id,.hljs-selector-pseudo,.hljs-title{color:#795da3}.hljs-addition{color:#55a532;background-color:#eaffea}.hljs-deletion{color:#bd2c00;background-color:#ffecec}.hljs-link{text-decoration:underline}.markdown-body{font-size:16px;line-height:1.5;word-wrap:break-word}.markdown-body:after,.markdown-body:before{display:table;content:""}.markdown-body:after{clear:both}.markdown-body>:first-child{margin-top:0!important}.markdown-body>:last-child{margin-bottom:0!important}.markdown-body a:not([href]){color:inherit;text-decoration:none}.markdown-body .absent{color:#c00}.markdown-body .anchor{float:left;padding-right:4px;margin-left:-20px;line-height:1}.markdown-body .anchor:focus{outline:none}.markdown-body blockquote,.markdown-body dl,.markdown-body ol,.markdown-body p,.markdown-body pre,.markdown-body table,.markdown-body ul{margin-top:0;margin-bottom:16px}.markdown-body hr{height:.25em;padding:0;margin:24px 0;background-color:#e7e7e7;border:0}.markdown-body blockquote{font-size:16px;padding:0 1em;color:#777;border-left:.25em solid #ddd}.markdown-body blockquote>:first-child{margin-top:0}.markdown-body blockquote>:last-child{margin-bottom:0}.markdown-body kbd,.popover kbd{display:inline-block;padding:3px 5px;font-size:11px;line-height:10px;color:#555;vertical-align:middle;background-color:#fcfcfc;border:1px solid #ccc;border-bottom-color:#bbb;border-radius:3px;box-shadow:inset 0 -1px 0 #bbb}.markdown-body .loweralpha{list-style-type:lower-alpha}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4,.markdown-body h5,.markdown-body h6{margin-top:24px;margin-bottom:16px;font-weight:600;line-height:1.25}.markdown-body h1 .octicon-link,.markdown-body h2 .octicon-link,.markdown-body h3 .octicon-link,.markdown-body h4 .octicon-link,.markdown-body h5 .octicon-link,.markdown-body h6 .octicon-link{color:#000;vertical-align:middle;visibility:hidden}.markdown-body h1:hover .anchor,.markdown-body h2:hover .anchor,.markdown-body h3:hover .anchor,.markdown-body h4:hover .anchor,.markdown-body h5:hover .anchor,.markdown-body h6:hover .anchor{text-decoration:none}.markdown-body h1:hover .anchor .octicon-link,.markdown-body h2:hover .anchor .octicon-link,.markdown-body h3:hover .anchor .octicon-link,.markdown-body h4:hover .anchor .octicon-link,.markdown-body h5:hover .anchor .octicon-link,.markdown-body h6:hover .anchor .octicon-link{visibility:visible}.markdown-body h1 code,.markdown-body h1 tt,.markdown-body h2 code,.markdown-body h2 tt,.markdown-body h3 code,.markdown-body h3 tt,.markdown-body h4 code,.markdown-body h4 tt,.markdown-body h5 code,.markdown-body h5 tt,.markdown-body h6 code,.markdown-body h6 tt{font-size:inherit}.markdown-body h1{font-size:2em}.markdown-body h1,.markdown-body h2{padding-bottom:.3em;border-bottom:1px solid #eee}.markdown-body h2{font-size:1.5em}.markdown-body h3{font-size:1.25em}.markdown-body h4{font-size:1em}.markdown-body h5{font-size:.875em}.markdown-body h6{font-size:.85em;color:#777}.markdown-body ol,.markdown-body ul{padding-left:2em}.markdown-body ol.no-list,.markdown-body ul.no-list{padding:0;list-style-type:none}.markdown-body ol ol,.markdown-body ol ul,.markdown-body ul ol,.markdown-body ul ul{margin-top:0;margin-bottom:0}.markdown-body li>p{margin-top:16px}.markdown-body li+li{padding-top:.25em}.markdown-body dl{padding:0}.markdown-body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:700}.markdown-body dl dd{padding:0 16px;margin-bottom:16px}.markdown-body table{display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}.markdown-body table th{font-weight:700}.markdown-body table td,.markdown-body table th{padding:6px 13px;border:1px solid #ddd}.markdown-body table tr{background-color:#fff;border-top:1px solid #ccc}.markdown-body table tr:nth-child(2n){background-color:#f8f8f8}.markdown-body img{max-width:100%;box-sizing:content-box;background-color:#fff}.markdown-body img[align=right]{padding-left:20px}.markdown-body img[align=left]{padding-right:20px}.markdown-body .emoji{max-width:none;vertical-align:text-top;background-color:transparent}.markdown-body span.frame{display:block;overflow:hidden}.markdown-body span.frame>span{display:block;float:left;width:auto;padding:7px;margin:13px 0 0;overflow:hidden;border:1px solid #ddd}.markdown-body span.frame span img{display:block;float:left}.markdown-body span.frame span span{display:block;padding:5px 0 0;clear:both;color:#333}.markdown-body span.align-center{display:block;overflow:hidden;clear:both}.markdown-body span.align-center>span{display:block;margin:13px auto 0;overflow:hidden;text-align:center}.markdown-body span.align-center span img{margin:0 auto;text-align:center}.markdown-body span.align-right{display:block;overflow:hidden;clear:both}.markdown-body span.align-right>span{display:block;margin:13px 0 0;overflow:hidden;text-align:right}.markdown-body span.align-right span img{margin:0;text-align:right}.markdown-body span.float-left{display:block;float:left;margin-right:13px;overflow:hidden}.markdown-body span.float-left span{margin:13px 0 0}.markdown-body span.float-right{display:block;float:right;margin-left:13px;overflow:hidden}.markdown-body span.float-right>span{display:block;margin:13px auto 0;overflow:hidden;text-align:right}.markdown-body code,.markdown-body tt{padding:0;padding-top:.2em;padding-bottom:.2em;margin:0;font-size:85%;background-color:rgba(0,0,0,.04);border-radius:3px}.markdown-body code:after,.markdown-body code:before,.markdown-body tt:after,.markdown-body tt:before{letter-spacing:-.2em;content:"\00a0"}.markdown-body code br,.markdown-body tt br{display:none}.markdown-body del code{text-decoration:inherit}.markdown-body pre{word-wrap:normal}.markdown-body pre>code{padding:0;margin:0;font-size:100%;word-break:normal;white-space:pre;background:transparent;border:0}.markdown-body .highlight{margin-bottom:16px}.markdown-body .highlight pre{margin-bottom:0;word-break:normal}.markdown-body .highlight pre,.markdown-body pre{padding:16px;overflow:auto;font-size:85%;line-height:1.45;background-color:#f7f7f7;border-radius:3px}.markdown-body pre code,.markdown-body pre tt{display:inline;max-width:auto;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}.markdown-body pre code:after,.markdown-body pre code:before,.markdown-body pre tt:after,.markdown-body pre tt:before{content:normal}.markdown-body .csv-data td,.markdown-body .csv-data th{padding:5px;overflow:hidden;font-size:12px;line-height:1;text-align:left;white-space:nowrap}.markdown-body .csv-data .blob-line-num{padding:10px 8px 9px;text-align:right;background:#fff;border:0}.markdown-body .csv-data tr{border-top:0}.markdown-body .csv-data th{font-weight:700;background:#f8f8f8;border-top:0}.news .alert .markdown-body blockquote{padding:0 0 0 40px;border:0 none}.activity-tab .news .alert .commits,.activity-tab .news .markdown-body blockquote{padding-left:0}.task-list-item{list-style-type:none}.task-list-item label{font-weight:400}.task-list-item.enabled label{cursor:pointer}.task-list-item+.task-list-item{margin-top:3px}.task-list-item-checkbox{float:left;margin:.31em 0 .2em -1.3em!important;vertical-align:middle;cursor:default!important}.markdown-body{padding-top:40px;padding-bottom:40px;max-width:758px;overflow:visible!important}.markdown-body .emoji{vertical-align:top}.markdown-body pre{border:inherit!important}.markdown-body code{color:inherit!important}.markdown-body pre code .wrapper{display:-moz-inline-flex;display:-ms-inline-flex;display:-o-inline-flex;display:inline-flex}.markdown-body pre code .gutter{float:left;overflow:hidden;-webkit-user-select:none;user-select:none}.markdown-body pre code .gutter.linenumber{text-align:right;position:relative;display:inline-block;cursor:default;z-index:4;padding:0 8px 0 0;min-width:20px;box-sizing:content-box;color:#afafaf!important;border-right:3px solid #6ce26c!important}.markdown-body pre code .gutter.linenumber>span:before{content:attr(data-linenumber)}.markdown-body pre code .code{float:left;margin:0 0 0 16px}.markdown-body .gist .line-numbers{border-left:none;border-top:none;border-bottom:none}.markdown-body .gist .line-data{border:none}.markdown-body .gist table{border-spacing:0;border-collapse:inherit!important}.markdown-body code[data-gist-id]{background:none;padding:0}.markdown-body code[data-gist-id]:after,.markdown-body code[data-gist-id]:before{content:""}.markdown-body code[data-gist-id] .blob-num{border:unset}.markdown-body code[data-gist-id] table{overflow:unset;margin-bottom:unset}.markdown-body code[data-gist-id] table tr{background:unset}.markdown-body[dir=rtl] pre{direction:ltr}.markdown-body[dir=rtl] code{direction:ltr;unicode-bidi:embed}.markdown-body .alert>p{margin-bottom:0}.markdown-body pre.abc,.markdown-body pre.flow-chart,.markdown-body pre.graphviz,.markdown-body pre.mermaid,.markdown-body pre.sequence-diagram,.markdown-body pre.vega{text-align:center;background-color:inherit;border-radius:0;white-space:inherit}.markdown-body pre.abc>code,.markdown-body pre.flow-chart>code,.markdown-body pre.graphviz>code,.markdown-body pre.mermaid>code,.markdown-body pre.sequence-diagram>code,.markdown-body pre.vega>code{text-align:left}.markdown-body pre.abc>svg,.markdown-body pre.flow-chart>svg,.markdown-body pre.graphviz>svg,.markdown-body pre.mermaid>svg,.markdown-body pre.sequence-diagram>svg,.markdown-body pre.vega>svg{max-width:100%;height:100%}.markdown-body pre>code.wrap{white-space:pre-wrap;white-space:-moz-pre-wrap;white-space:-pre-wrap;white-space:-o-pre-wrap;word-wrap:break-word}.markdown-body .alert>p,.markdown-body .alert>ul{margin-bottom:0}.markdown-body summary{display:list-item}.markdown-body summary:focus{outline:none}.markdown-body details summary{cursor:pointer}.markdown-body details:not([open])>:not(summary){display:none}.markdown-body figure{margin:1em 40px}.markdown-body .mark,.markdown-body mark{background-color:#fff1a7}.vimeo,.youtube{cursor:pointer;display:table;text-align:center;background-position:50%;background-repeat:no-repeat;background-size:contain;background-color:#000;overflow:hidden}.vimeo,.youtube{position:relative;width:100%}.youtube{padding-bottom:56.25%}.vimeo img{width:100%;object-fit:contain;z-index:0}.youtube img{object-fit:cover;z-index:0}.vimeo iframe,.youtube iframe,.youtube img{width:100%;height:100%;position:absolute;top:0;left:0}.vimeo iframe,.youtube iframe{vertical-align:middle;z-index:1}.vimeo .icon,.youtube .icon{position:absolute;height:auto;width:auto;top:50%;left:50%;transform:translate(-50%,-50%);color:#fff;opacity:.3;transition:opacity .2s;z-index:0}.vimeo:hover .icon,.youtube:hover .icon{opacity:.6;transition:opacity .2s}.slideshare .inner,.speakerdeck .inner{position:relative;width:100%}.slideshare .inner iframe,.speakerdeck .inner iframe{position:absolute;top:0;bottom:0;left:0;right:0;width:100%;height:100%}.MJX_Assistive_MathML{display:none}.ui-infobar{position:relative;z-index:2;max-width:760px;margin:25px auto -25px;padding:0 15px;color:#777}.toc .invisable-node{list-style-type:none}.ui-toc{position:fixed;bottom:20px;z-index:998}.ui-toc-label{opacity:.3;background-color:#ccc;border:none;transition:opacity .2s}.ui-toc .open .ui-toc-label{opacity:1;color:#fff;transition:opacity .2s}.ui-toc-label:focus{opacity:.3;background-color:#ccc;color:#000}.ui-toc-label:hover{opacity:1;background-color:#ccc;transition:opacity .2s}.ui-toc-dropdown{margin-top:23px;margin-bottom:20px;padding-left:10px;padding-right:10px;max-width:45vw;width:25vw;max-height:70vh;overflow:auto;text-align:inherit}.ui-toc-dropdown>.toc{max-height:calc(70vh - 100px);overflow:auto}.ui-toc-dropdown[dir=rtl] .nav{padding-right:0;letter-spacing:.0029em}.ui-toc-dropdown a{overflow:hidden;text-overflow:ellipsis;white-space:pre}.ui-toc-dropdown .nav>li>a{display:block;padding:4px 20px;font-size:13px;font-weight:500;color:#767676}.ui-toc-dropdown .nav>li:first-child:last-child > ul,.ui-toc-dropdown .toc.expand ul{display:block}.ui-toc-dropdown .nav>li>a:focus,.ui-toc-dropdown .nav>li>a:hover{padding-left:19px;color:#000;text-decoration:none;background-color:transparent;border-left:1px solid #000}.ui-toc-dropdown[dir=rtl] .nav>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav>li>a:hover{padding-right:19px;border-left:none;border-right:1px solid #000}.ui-toc-dropdown .nav>.active:focus>a,.ui-toc-dropdown .nav>.active:hover>a,.ui-toc-dropdown .nav>.active>a{padding-left:18px;font-weight:700;color:#000;background-color:transparent;border-left:2px solid #000}.ui-toc-dropdown[dir=rtl] .nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav>.active:hover>a,.ui-toc-dropdown[dir=rtl] .nav>.active>a{padding-right:18px;border-left:none;border-right:2px solid #000}.ui-toc-dropdown .nav .nav{display:none;padding-bottom:10px}.ui-toc-dropdown .nav>.active>ul{display:block}.ui-toc-dropdown .nav .nav>li>a{padding-top:1px;padding-bottom:1px;padding-left:30px;font-size:12px;font-weight:400}.ui-toc-dropdown[dir=rtl] .nav .nav>li>a{padding-right:30px}.ui-toc-dropdown .nav .nav>li>ul>li>a{padding-top:1px;padding-bottom:1px;padding-left:40px;font-size:12px;font-weight:400}.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>a{padding-right:40px}.ui-toc-dropdown .nav .nav>li>a:focus,.ui-toc-dropdown .nav .nav>li>a:hover{padding-left:29px}.ui-toc-dropdown[dir=rtl] .nav .nav>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav .nav>li>a:hover{padding-right:29px}.ui-toc-dropdown .nav .nav>li>ul>li>a:focus,.ui-toc-dropdown .nav .nav>li>ul>li>a:hover{padding-left:39px}.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>a:hover{padding-right:39px}.ui-toc-dropdown .nav .nav>.active:focus>a,.ui-toc-dropdown .nav .nav>.active:hover>a,.ui-toc-dropdown .nav .nav>.active>a{padding-left:28px;font-weight:500}.ui-toc-dropdown[dir=rtl] .nav .nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active:hover>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>a{padding-right:28px}.ui-toc-dropdown .nav .nav>.active>.nav>.active:focus>a,.ui-toc-dropdown .nav .nav>.active>.nav>.active:hover>a,.ui-toc-dropdown .nav .nav>.active>.nav>.active>a{padding-left:38px;font-weight:500}.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active:hover>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active>a{padding-right:38px}.markdown-body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,sans-serif}html[lang^=ja] .markdown-body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,Hiragino Kaku Gothic Pro,ヒラギノ角ゴ Pro W3,Osaka,Meiryo,メイリオ,MS Gothic,ＭＳ\ ゴシック,sans-serif}html[lang=zh-tw] .markdown-body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,PingFang TC,Microsoft JhengHei,微軟正黑,sans-serif}html[lang=zh-cn] .markdown-body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,PingFang SC,Microsoft YaHei,微软雅黑,sans-serif}html .markdown-body[lang^=ja]{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,Hiragino Kaku Gothic Pro,ヒラギノ角ゴ Pro W3,Osaka,Meiryo,メイリオ,MS Gothic,ＭＳ\ ゴシック,sans-serif}html .markdown-body[lang=zh-tw]{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,PingFang TC,Microsoft JhengHei,微軟正黑,sans-serif}html .markdown-body[lang=zh-cn]{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,PingFang SC,Microsoft YaHei,微软雅黑,sans-serif}html[lang^=ja] .ui-toc-dropdown{font-family:Source Sans Pro,Helvetica,Arial,Meiryo UI,MS PGothic,ＭＳ\ Ｐゴシック,sans-serif}html[lang=zh-tw] .ui-toc-dropdown{font-family:Source Sans Pro,Helvetica,Arial,Microsoft JhengHei UI,微軟正黑UI,sans-serif}html[lang=zh-cn] .ui-toc-dropdown{font-family:Source Sans Pro,Helvetica,Arial,Microsoft YaHei UI,微软雅黑UI,sans-serif}html .ui-toc-dropdown[lang^=ja]{font-family:Source Sans Pro,Helvetica,Arial,Meiryo UI,MS PGothic,ＭＳ\ Ｐゴシック,sans-serif}html .ui-toc-dropdown[lang=zh-tw]{font-family:Source Sans Pro,Helvetica,Arial,Microsoft JhengHei UI,微軟正黑UI,sans-serif}html .ui-toc-dropdown[lang=zh-cn]{font-family:Source Sans Pro,Helvetica,Arial,Microsoft YaHei UI,微软雅黑UI,sans-serif}.ui-affix-toc{position:fixed;top:0;max-width:15vw;max-height:70vh;overflow:auto}.back-to-top,.expand-toggle,.go-to-bottom{display:block;padding:4px 10px;margin-top:10px;margin-left:10px;font-size:12px;font-weight:500;color:#999}.back-to-top:focus,.back-to-top:hover,.expand-toggle:focus,.expand-toggle:hover,.go-to-bottom:focus,.go-to-bottom:hover{color:#563d7c;text-decoration:none}.back-to-top,.go-to-bottom{margin-top:0}.ui-user-icon{width:20px;height:20px;display:block;border-radius:3px;margin-top:2px;margin-bottom:2px;margin-right:5px;background-position:50%;background-repeat:no-repeat;background-size:cover}.ui-user-icon.small{width:18px;height:18px;display:inline-block;vertical-align:middle;margin:0 0 .2em}.ui-infobar>small>span{line-height:22px}.ui-infobar>small .dropdown{display:inline-block}.ui-infobar>small .dropdown a:focus,.ui-infobar>small .dropdown a:hover{text-decoration:none}.ui-published-note{color:#337ab7}.ui-published-note .fa{font-size:20px;vertical-align:top}.unselectable{-webkit-user-select:none;-o-user-select:none;user-select:none}@media print{blockquote,div,img,pre,table{page-break-inside:avoid!important}a[href]:after{font-size:12px!important}}.markdown-body.slides{position:relative;z-index:1;color:#222}.markdown-body.slides:before{content:"";display:block;position:absolute;top:0;left:0;right:0;bottom:0;z-index:-1;background-color:currentColor;box-shadow:0 0 0 50vw}.markdown-body.slides section[data-markdown]{position:relative;margin-bottom:1.5em;background-color:#fff;text-align:center}.markdown-body.slides section[data-markdown] code{text-align:left}.markdown-body.slides section[data-markdown]:before{content:"";display:block;padding-bottom:56.23%}.markdown-body.slides section[data-markdown]>div:first-child{position:absolute;top:50%;left:1em;right:1em;transform:translateY(-50%);max-height:100%;overflow:hidden}.markdown-body.slides section[data-markdown]>ul{display:inline-block}.markdown-body.slides>section>section+section:after{content:"";position:absolute;top:-1.5em;right:1em;height:1.5em;border:3px solid #777}body{font-smoothing:subpixel-antialiased!important;-webkit-font-smoothing:subpixel-antialiased!important;-moz-osx-font-smoothing:auto!important;text-shadow:0 0 1em transparent,1px 1px 1.2px rgba(0,0,0,.004);-webkit-overflow-scrolling:touch;letter-spacing:.025em}.focus,:focus{outline:none!important}::-moz-focus-inner{border:0!important}body{font-family:Source Sans Pro,Helvetica,Arial,sans-serif}html[lang^=ja] body{font-family:Source Sans Pro,Helvetica,Arial,Hiragino Kaku Gothic Pro,ヒラギノ角ゴ Pro W3,Osaka,Meiryo,メイリオ,MS Gothic,ＭＳ\ ゴシック,sans-serif}html[lang=zh-tw] body{font-family:Source Sans Pro,Helvetica,Arial,PingFang TC,Microsoft JhengHei,微軟正黑,sans-serif}html[lang=zh-cn] body{font-family:Source Sans Pro,Helvetica,Arial,PingFang SC,Microsoft YaHei,微软雅黑,sans-serif}abbr[title]{border-bottom:none;text-decoration:underline;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}abbr[data-original-title],abbr[title]{cursor:help}body.modal-open{overflow-y:auto;padding-right:0!important}
    </style>
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    	<script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js" integrity="sha256-3Jy/GbSLrg0o9y5Z5n1uw0qxZECH7C6OQpVBgNFYa0g=" crossorigin="anonymous"></script>
    	<script src="https://cdnjs.cloudflare.com/ajax/libs/respond.js/1.4.2/respond.min.js" integrity="sha256-g6iAfvZp+nDQ2TdTR/VVKJf3bGro4ub5fvWSWVRi2NE=" crossorigin="anonymous"></script>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.9/es5-shim.min.js" integrity="sha256-8E4Is26QH0bD52WoQpcB+R/tcWQtpzlCojrybUd7Mxo=" crossorigin="anonymous"></script>
    <![endif]-->
</head>

<body>
    <div id="doc" class="markdown-body container-fluid comment-enabled" data-hard-breaks="true" style="position: relative;"><h1 id="Deep-Learning-Image-Captioning-Project-Documentation"><a class="anchor hidden-xs" href="#Deep-Learning-Image-Captioning-Project-Documentation" title="Deep-Learning-Image-Captioning-Project-Documentation"><span class="octicon octicon-link"></span></a>Deep Learning Image Captioning Project Documentation</h1><h2 id="Folder-Structure"><a class="anchor hidden-xs" href="#Folder-Structure" title="Folder-Structure"><span class="octicon octicon-link"></span></a>Folder Structure</h2><pre><code>├───{extracted folder name}
│    ├───datasets
│    │    ├───annotations
│    │    ├───test2014
│    │    ├───train2014
│    │    └───val2014
│    ├───utils
│    │    ├───models.py
│    │    └───data_loader.py
│    ├───scratch
│    │    ├───models
│    │         ├───final_coop-generator-3-25000.pkl
│    │         └───final_coop-discriminator-3-25000.pkl
│    ├───main.py
│    ├───GUI.py
│    ├───vocabulary.py
│    ├───vocab.pkl
│    ├───requirements.txt
│    ├───VideoDL.mp4
│    ├───vocab.pkl
│    ├───Notebook main latest training with loss progress.ipynb
│    ├───Notebook main latest training with testing.ipynb
</code></pre><h2 id="Installation"><a class="anchor hidden-xs" href="#Installation" title="Installation"><span class="octicon octicon-link"></span></a>Installation</h2><p>Python version: 3.6<br>
Torch version: 1.2.0<br>
Trained on the SUTD GPU Cluster<br>
<code>!pip install pycocotools</code><br>
<code>!pip install git+https://github.com/salaniz/pycocoevalcap</code><br>
<code>!pip install &gt; requirements.txt</code></p><pre><code class="python hljs"><span class="hljs-keyword">import</span> nltk
nltk.download(<span class="hljs-string">'punkt'</span>)
</code></pre><h2 id="Execution"><a class="anchor hidden-xs" href="#Execution" title="Execution"><span class="octicon octicon-link"></span></a>Execution</h2><h3 id="Training"><a class="anchor hidden-xs" href="#Training" title="Training"><span class="octicon octicon-link"></span></a>Training</h3><p>To retrain the model:</p><ul>
<li>
<p>Linux Users can execute the following:</p>
<p><code>python3 main.py</code></p>
</li>
<li>
<p>Windows Users can execute the following:</p>
<p><code>python main.py</code></p>
</li>
</ul><p>To execute the GUI:</p><ul>
<li>
<p>Linux Users can execute the following:</p>
<p><code>python3 GUI.py</code></p>
</li>
<li>
<p>Windows Users can execute the following:</p>
<p><code>python GUI.py</code></p>
</li>
</ul><h2 id="Model-Design"><a class="anchor hidden-xs" href="#Model-Design" title="Model-Design"><span class="octicon octicon-link"></span></a>Model Design</h2><p>Cooperative Image Captioning<br>
<img src="https://i.imgur.com/2nsH5Ll.png" alt=""><br>
<img src="https://imgur.com/jttTYaF.png" alt=""></p><p>Motivation:<br>
Not only do we want to create realistic captions, but also to create captions unique to the input image.<br>
Hence, we needed to introduce a form of discriminative loss to the provided tutorial image captioning model.</p><h2 id="Architecture"><a class="anchor hidden-xs" href="#Architecture" title="Architecture"><span class="octicon octicon-link"></span></a>Architecture</h2><p>The architecture is heavily inspired by the following paper:</p><p><a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Vered_Joint_Optimization_for_Cooperative_Image_Captioning_ICCV_2019_paper.pdf" target="_blank" rel="noopener">Gilad Vered, Gal Oren, Yuval Atzmon, Gal Chechik; The IEEE International Conference on Computer Vision (ICCV), 2019, pp. 8898-8907</a></p><h2 id="Training-Hyperparameters"><a class="anchor hidden-xs" href="#Training-Hyperparameters" title="Training-Hyperparameters"><span class="octicon octicon-link"></span></a>Training Hyperparameters</h2><p>Batch size = 256<br>
Epochs = 3<br>
Embedding size = 300<br>
Context size = 300<br>
Hidden size = 512</p><h1 id="Image-Pre-Processing-for-training"><a class="anchor hidden-xs" href="#Image-Pre-Processing-for-training" title="Image-Pre-Processing-for-training"><span class="octicon octicon-link"></span></a>Image Pre-Processing for training</h1><pre><code class="python hljs">transform_train = transforms.Compose([ 
    transforms.Resize(<span class="hljs-number">256</span>),                          <span class="hljs-comment"># smaller edge of image resized to 256</span>
    transforms.RandomCrop(<span class="hljs-number">224</span>),                      <span class="hljs-comment"># get 224x224 crop from random location</span>
    transforms.RandomHorizontalFlip(),               <span class="hljs-comment"># horizontally flip image with probability=0.5</span>
    transforms.ToTensor(),                           <span class="hljs-comment"># convert the PIL Image to a tensor</span>
    transforms.Normalize((<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>),      <span class="hljs-comment"># normalize image for pre-trained model</span>
                         (<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>))])
</code></pre><p>Training set is pre-processed firstly by resizing each image to 256, doing random crops of size 224, and doing random horizontal flip on the images. Once the images are pre-processed, they are then converted into tensors and normalised using the following values:</p><ul>
<li>mean: (0.485, 0.456, 0.406)</li>
<li>std : (0.229, 0.224, 0.225)</li>
</ul><h2 id="Vocabulary-Preprocessing"><a class="anchor hidden-xs" href="#Vocabulary-Preprocessing" title="Vocabulary-Preprocessing"><span class="octicon octicon-link"></span></a>Vocabulary Preprocessing</h2><p>Vocabulary preprocessing is heavily inspired by the Pytorch Tutorial with some changes specific to the dataset.</p><p><a href="https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/image_captioning/build_vocab.py" target="_blank" rel="noopener">Yunjey Choi Pytorch Image Captioning Tutorial</a></p><h2 id="Models"><a class="anchor hidden-xs" href="#Models" title="Models"><span class="octicon octicon-link"></span></a>Models</h2><h3 id="EncoderCNN-Model"><a class="anchor hidden-xs" href="#EncoderCNN-Model" title="EncoderCNN-Model"><span class="octicon octicon-link"></span></a>EncoderCNN Model</h3><pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">EncoderCNN</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, embed_size)</span>:</span>
        super(EncoderCNN, self).__init__()
        resnet = models.resnet50(pretrained=<span class="hljs-keyword">True</span>)
        <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> resnet.parameters():
            param.requires_grad_(<span class="hljs-keyword">False</span>)
        
        modules = list(resnet.children())[:<span class="hljs-number">-1</span>]
        self.resnet = nn.Sequential(*modules)
        self.embed = nn.Linear(resnet.fc.in_features, embed_size)
        self.train_params = list(self.embed.parameters())

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, images)</span>:</span>
        features = self.resnet(images)
        features = features.view(features.size(<span class="hljs-number">0</span>), <span class="hljs-number">-1</span>)
        features = self.embed(features)
        <span class="hljs-keyword">return</span> features
</code></pre><p>Get image features using a resnet50 backbone similar to how it is done in the Pytorch Image Captioning Tutorial. The EncoderCNN takes in an image, gets the features of the image and embed the features using a fully-connected layer for the decoderRNN model. This model is part of the Generator model explained in the following sections.</p><h3 id="DecoderRNN-Model"><a class="anchor hidden-xs" href="#DecoderRNN-Model" title="DecoderRNN-Model"><span class="octicon octicon-link"></span></a>DecoderRNN Model</h3><pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DecoderRNN</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, embed_size, hidden_size, vocab_size, num_layers=<span class="hljs-number">2</span>)</span>:</span>
        super().__init__()
        self.embedding_layer = nn.Embedding(vocab_size, embed_size)
        
        self.lstm = nn.LSTM(input_size = embed_size,hidden_size = hidden_size,
                            num_layers = num_layers, batch_first = <span class="hljs-keyword">True</span>)
        
        self.linear = nn.Linear(hidden_size, vocab_size)
        self.train_params = list(self.parameters())
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, features, captions)</span>:</span>
        captions = captions[:, :<span class="hljs-number">-1</span>]
        embed = self.embedding_layer(captions)
        embed = torch.cat((features.unsqueeze(<span class="hljs-number">1</span>), embed), dim = <span class="hljs-number">1</span>)
        lstm_outputs, _ = self.lstm(embed)
        out = self.linear(lstm_outputs)
        
        <span class="hljs-keyword">return</span> out

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sample</span><span class="hljs-params">(self, inputs, states=None, max_len=<span class="hljs-number">20</span>)</span>:</span>
        <span class="hljs-string">" accepts pre-processed image tensor (inputs) and returns predicted sentence (list of tensor ids of length max_len) "</span>
        output_sentence = []
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(max_len):
            lstm_outputs, states = self.lstm(inputs, states)
            lstm_outputs = lstm_outputs.squeeze(<span class="hljs-number">1</span>)
            out = self.linear(lstm_outputs)
            last_pick = out.max(<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>]
            output_sentence.append(last_pick.item())
            inputs = self.embedding_layer(last_pick).unsqueeze(<span class="hljs-number">1</span>)
        
        <span class="hljs-keyword">return</span> output_sentence
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">beam_sample</span><span class="hljs-params">(self, inputs, states=None, max_len=<span class="hljs-number">20</span>, k=<span class="hljs-number">1</span>)</span>:</span>
        <span class="hljs-string">" accepts pre-processed image tensor (inputs) and returns predicted sentence (list of tensor ids of length max_len) "</span>
        possible_seq = [(<span class="hljs-number">1</span>, inputs, states)]
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(max_len):
            to_pick = []
            <span class="hljs-keyword">for</span> probs,seq,states <span class="hljs-keyword">in</span> possible_seq:
                inputs = self.embedding_layer(seq[<span class="hljs-number">-1</span>])
                lstm_outputs, states = self.lstm(inputs, states)
                out = self.linear(lstm_outputs).squeeze(<span class="hljs-number">0</span>)
                sorted_out, indices = torch.sort(out, <span class="hljs-number">1</span>)
                
                <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(k):
                    to_pick.append((probs + nn.functional.log_softmax(sorted_out[i]), inputs + [indices[i]], states) )
                 
            to_pick.sort(reverse=<span class="hljs-keyword">True</span>)
            possible_seq = to_pick[:k]
            
        <span class="hljs-keyword">return</span> to_pick[<span class="hljs-number">0</span>]
</code></pre><p>DecoderRNN is part of the generator model, explained in the following sections, that takes in the features from the encoder and processes it for the caption generation. This model is designed using an LSTM layer to associate the features of image with the embedded captions. This model outputs unnormalised probability of a batch of captions. The unnormalised probability will then be used for further processing as part of the generator model.</p><h3 id="EncoderRNN-Model"><a class="anchor hidden-xs" href="#EncoderRNN-Model" title="EncoderRNN-Model"><span class="octicon octicon-link"></span></a>EncoderRNN Model</h3><pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">EncoderRNN</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, embed_size, hidden_size, vocab_size, context_size, num_layers=<span class="hljs-number">1</span>)</span>:</span>
        super().__init__()
        self.lstm = nn.LSTM(input_size = embed_size,hidden_size = hidden_size,
                            num_layers = num_layers, batch_first = <span class="hljs-keyword">True</span>)
        self.linear = nn.Linear(hidden_size, context_size)
        self.train_params = list(self.parameters())
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, captions)</span>:</span>
        lstm_outputs, _ = self.lstm(captions)
        out = self.linear(lstm_outputs[:,<span class="hljs-number">-1</span>,:].squeeze(<span class="hljs-number">1</span>))
        <span class="hljs-keyword">return</span> out
</code></pre><p>EncoderRNN is part of the discriminator model which is used to compute the caption feature vectors to calculate the similarity between the generated captions and the ground truth captions. This model takes in the generated captions from the generator model and outputs the caption features vectors to be used for the training.</p><h3 id="Generator-Model"><a class="anchor hidden-xs" href="#Generator-Model" title="Generator-Model"><span class="octicon octicon-link"></span></a>Generator Model</h3><p><img src="https://i.imgur.com/2nsH5Ll.png" alt=""><br>
Generator takes in the <code>Extracted Features</code> and <code>Caption Features</code> from EncorderCNN and DecoderRNN(LSTM) respectively.</p><p>It outputs a generated probability of caption word and the extracted image features. These values will then be passed to and trained with the Discriminator.</p><h3 id="Discriminator-Model"><a class="anchor hidden-xs" href="#Discriminator-Model" title="Discriminator-Model"><span class="octicon octicon-link"></span></a>Discriminator Model</h3><p><img src="https://imgur.com/jttTYaF.png" alt=""><br>
The FC layer acts like an Embedding layer, but we needed to allow the model to take in non-discrete inputs.</p><p>The method of sampling for the discriminator follow the Cooperative Image Captioning model, which uses Partial-Sampling-Straight-Through (PSST). Motivation for this approach is to avoid the main issues regarding sampling for text generation.</p><p>Sampling for text generation usually involves using argmax, which is not differentiable. This can be avoided by using the Gumbel Softmax function which samples in a differentiable manner. However, Gumbel Softmax has high variance and bias, as described in the Cooperative Image Captioning paper. So instead, the paper proposes PSST, which passes the full continuous values from the Generator to the Discriminator with probability rho (rho = 0.25), and passes a 1-hot representation using Gumbel Softmax with probability 1-rho.</p><p>Next, the generated caption (FC Layer) passes through the EncoderRNN, which computes the caption features vector. We then use compute the pairwise cosine similarity between the image features vectors and caption features vectors which we pass to the loss function to minimise.</p><h2 id="Loss-Function"><a class="anchor hidden-xs" href="#Loss-Function" title="Loss-Function"><span class="octicon octicon-link"></span></a>Loss Function</h2><pre><code class="python hljs">        max_loss_1 = <span class="hljs-number">0</span>
        max_loss_2 = <span class="hljs-number">0</span>
        pos_pair_term = cosineSim(cap_feats[<span class="hljs-number">0</span>], img_feats[<span class="hljs-number">0</span>])
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>,batch_size):
            max_loss_1 += max(<span class="hljs-number">0</span>, <span class="hljs-number">1</span> - pos_pair_term + cosineSim(cap_feats[<span class="hljs-number">0</span>], img_feats[i]))
            max_loss_2 += max(<span class="hljs-number">0</span>, <span class="hljs-number">1</span> - pos_pair_term + cosineSim(cap_feats[i], img_feats[<span class="hljs-number">0</span>]))
        
        
        
        <span class="hljs-comment"># Calculate the batch loss.</span>
        loss_A = criterion_A(out.view(<span class="hljs-number">-1</span>, vocab_size), captions.view(<span class="hljs-number">-1</span>))
        loss_B = (max_loss_1 + max_loss_2)/(batch_size<span class="hljs-number">-1</span>)
        
        B_weight = <span class="hljs-number">0.5</span>
        loss = loss_A + B_weight*loss_B
</code></pre><p>Following a similar approach to the Cooperative Image Captioning model, our loss is based on 2 components: Natural Loss and Discriminability Loss.</p><p>For the Natural Loss, we used the CrossEntropyLoss method to compute the perplexity of the model given the ground truth caption, which is a measure of how well a probability model predicts a sample. This loss is used to ensure the language of the captions are realistic.</p><p>For the Discriminability Loss, we aim to have generated captions unique and specific to the image. To do this, we compute the similarity of a candidate caption and its source image and similarity of that candidate caption and distractor images. We aim to maximise the similarity of the generated caption and the source image, while minimising the similarity with the distractor images.</p><p>This would increase the model’s ability to perform in self-retrieval, where given a set of images (distractor images + source image) and the generated caption, one can identify the source image from the set of distractor images. A similar logic is applied to identify the correct caption given the source image and a set of captions (distractor captions + generated caption).</p><h2 id="Performance"><a class="anchor hidden-xs" href="#Performance" title="Performance"><span class="octicon octicon-link"></span></a>Performance</h2><p>The model is evaluated using the validation set (<code>val2014</code>).</p><p>The following are the sample images generated.<br>
<img src="https://imgur.com/Ed4l7Jl.jpg" alt=""></p><p>Using Bleu-1, Bleu-2, Bleu-3, Bleu-4, and ROUGE-L  evaluation methods,<br>
the following are some samples of the prediction scores:<br>
<img src="https://i.imgur.com/KQjYpHU.png" alt=""><br>
<img src="https://i.imgur.com/C9wYj14.png" alt=""><br>
<img src="https://i.imgur.com/jPQsWRU.png" alt=""></p><p>Sample Captions:</p><pre><code>1. ('a bench on a hill overlooking a body of water',)
    A person riding a surfboard on a beach .
2. ('A street is empty and the traffic light is red.',)
    A street sign with a traffic light in the background.
3. ('A red cross sitting over a white toilet.',)
    A bathroom with a toilet and a sink in it on a white.

</code></pre><p>Average scores:</p><pre><code>	bleu1 = 0.32000085354820995
	bleu2 = 0.1555914132856976
	bleu3 = 0.04849434560587178
	bleu4 = 0.025579018256153007
	rougel =  0.3220487108487605
</code></pre><h2 id="GUI"><a class="anchor hidden-xs" href="#GUI" title="GUI"><span class="octicon octicon-link"></span></a>GUI</h2><p>After training the models, we implemented a GUI based off python’s tkinter library.</p><pre><code class="python hljs"><span class="hljs-keyword">from</span> tkinter <span class="hljs-keyword">import</span> *
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image, ImageTk

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
    root = Toplevel()
    app = Window(root)
</code></pre><p>Dynamic Image Resizing has been implemented.</p><pre><code class="python hljs">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">resizeimg</span><span class="hljs-params">(self, img)</span>:</span>
        basewidth = <span class="hljs-number">500</span>
        wpercent = (basewidth/float(img.size[<span class="hljs-number">0</span>]))
        hsize = int((float(img.size[<span class="hljs-number">1</span>])*float(wpercent)))
        img = img.resize((basewidth,hsize), Image.ANTIALIAS)
        <span class="hljs-keyword">return</span> img
</code></pre><p>The GUI calls the Generator, data_loader modules.</p><pre><code class="python hljs">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">initmodels</span><span class="hljs-params">(self)</span>:</span>
        transform_train = transforms.Compose([ 
        transforms.Resize(<span class="hljs-number">256</span>),                          <span class="hljs-comment"># smaller edge of image resized to 256</span>
        transforms.RandomCrop(<span class="hljs-number">224</span>),                      <span class="hljs-comment"># get 224x224 crop from random location</span>
        transforms.RandomHorizontalFlip(),               <span class="hljs-comment"># horizontally flip image with probability=0.5</span>
        transforms.ToTensor(),                           <span class="hljs-comment"># convert the PIL Image to a tensor</span>
        transforms.Normalize((<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>),      <span class="hljs-comment"># normalize image for pre-trained model</span>
                             (<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>))])
        self.testdata_loader = get_loader(transform=transform_train,
                             mode=<span class="hljs-string">'test'</span>,
                             batch_size=<span class="hljs-number">1</span>,
                             vocab_threshold=<span class="hljs-number">5</span>,
                             vocab_from_file=<span class="hljs-keyword">True</span>)

        embed_size = <span class="hljs-number">300</span>           <span class="hljs-comment"># dimensionality of image and word embeddings</span>
        hidden_size = <span class="hljs-number">512</span>          <span class="hljs-comment"># number of features in hidden state of the RNN decoder</span>
        vocab_size = len(self.testdata_loader.dataset.vocab)
        self.Generator = Generator(embed_size, hidden_size, vocab_size, embed_size)
        self.Generator.load_state_dict(torch.load(<span class="hljs-string">'./models/decoder-1-26500.pkl'</span>))
        self.encoder = self.Generator.cnn
        self.decoder = self.Generator.rnn
        self.encoder.to(device)
        self.decoder.to(device)
        self.encoder.eval()
        self.decoder.eval()
</code></pre><p>Some good predictions:<br>
<img src="https://i.imgur.com/E9B5NJA.jpg" alt=""><br>
<img src="https://i.imgur.com/oww09ZJ.jpg" alt=""><br>
<img src="https://i.imgur.com/TQFwslW.jpg" alt=""><br>
<img src="https://i.imgur.com/nT2nLjt.jpg" alt=""><br>
<img src="https://i.imgur.com/1kgsSke.png" alt=""></p><p>Some poor predictions:<br>
<img src="https://i.imgur.com/TJZ5VhX.jpg" alt=""><br>
<img src="https://i.imgur.com/5Et8Awl.jpg" alt=""><br>
<img src="https://i.imgur.com/FMYoxeA.png" alt=""></p><h2 id="Group-Members-Contributions"><a class="anchor hidden-xs" href="#Group-Members-Contributions" title="Group-Members-Contributions"><span class="octicon octicon-link"></span></a>Group Members Contributions</h2><p>Gong Chen           1002870<br>
Ivan Christian      1003056<br>
Lim Theck Sean      1002777<br>
Tang Mingzheng Paul 1002768</p><ul>
<li>Report : Everyone</li>
<li>EncoderCNN : Ivan</li>
<li>EncoderRNN : Gong Chen</li>
<li>DecoderRNN : Ivan</li>
<li>Generator : Paul</li>
<li>Disciminator : Paul</li>
<li>Train Function : Ivan</li>
<li>Test Function : Gong Chen</li>
<li>Custom Loss Function : Paul</li>
<li>GUI : Sean</li>
</ul><h2 id="WeirdestFunniest-Captions"><a class="anchor hidden-xs" href="#WeirdestFunniest-Captions" title="WeirdestFunniest-Captions"><span class="octicon octicon-link"></span></a>Weirdest/Funniest Captions</h2><p>Our trained models can also be many different things!</p><ol>
<li>
<p>An Otaku!<br>
<img src="https://i.imgur.com/BHK7rJb.jpg" alt=""></p>
</li>
<li>
<p>An Architecture Connoisseur~<br>
<img src="https://i.imgur.com/iQ7dKxt.jpg" alt=""></p>
</li>
<li>
<p>A Colour Blind &gt;.&lt;<br>
<img src="https://i.imgur.com/BoikywK.jpg" alt=""></p>
</li>
<li>
<p>SUTD 50.039 Students…<br>
<img src="https://i.imgur.com/JMtUxpE.jpg" alt=""></p>
</li>
<li>
<p>The Bathroom Pope</p>
</li>
</ol><p><img src="https://i.imgur.com/Yi0ouNY.png" alt=""></p></div>
    <div class="ui-toc dropup unselectable hidden-print" style="display:none;">
        <div class="pull-right dropdown">
            <a id="tocLabel" class="ui-toc-label btn btn-default" data-toggle="dropdown" href="#" role="button" aria-haspopup="true" aria-expanded="false" title="Table of content">
                <i class="fa fa-bars"></i>
            </a>
            <ul id="ui-toc" class="ui-toc-dropdown dropdown-menu" aria-labelledby="tocLabel">
                <div class="toc"><ul class="nav">
<li class=""><a href="#Deep-Learning-Image-Captioning-Project-Documentation" title="Deep Learning Image Captioning Project Documentation">Deep Learning Image Captioning Project Documentation</a><ul class="nav">
<li><a href="#Folder-Structure" title="Folder Structure">Folder Structure</a></li>
<li><a href="#Installation" title="Installation">Installation</a></li>
<li><a href="#Execution" title="Execution">Execution</a><ul class="nav">
<li><a href="#Training" title="Training">Training</a></li>
</ul>
</li>
<li><a href="#Model-Design" title="Model Design">Model Design</a></li>
<li><a href="#Architecture" title="Architecture">Architecture</a></li>
<li><a href="#Training-Hyperparameters" title="Training Hyperparameters">Training Hyperparameters</a></li>
</ul>
</li>
<li class=""><a href="#Image-Pre-Processing-for-training" title="Image Pre-Processing for training">Image Pre-Processing for training</a><ul class="nav">
<li><a href="#Vocabulary-Preprocessing" title="Vocabulary Preprocessing">Vocabulary Preprocessing</a></li>
<li><a href="#Models" title="Models">Models</a><ul class="nav">
<li><a href="#EncoderCNN-Model" title="EncoderCNN Model">EncoderCNN Model</a></li>
<li><a href="#DecoderRNN-Model" title="DecoderRNN Model">DecoderRNN Model</a></li>
<li><a href="#EncoderRNN-Model" title="EncoderRNN Model">EncoderRNN Model</a></li>
<li><a href="#Generator-Model" title="Generator Model">Generator Model</a></li>
<li><a href="#Discriminator-Model" title="Discriminator Model">Discriminator Model</a></li>
</ul>
</li>
<li><a href="#Loss-Function" title="Loss Function">Loss Function</a></li>
<li><a href="#Performance" title="Performance">Performance</a></li>
<li><a href="#GUI" title="GUI">GUI</a></li>
<li><a href="#Group-Members-Contributions" title="Group Members Contributions">Group Members Contributions</a></li>
<li class=""><a href="#WeirdestFunniest-Captions" title="Weirdest/Funniest Captions">Weirdest/Funniest Captions</a></li>
</ul>
</li>
</ul>
</div><div class="toc-menu"><a class="expand-toggle" href="#">Expand all</a><a class="back-to-top" href="#">Back to top</a><a class="go-to-bottom" href="#">Go to bottom</a></div>
            </ul>
        </div>
    </div>
    <div id="ui-toc-affix" class="ui-affix-toc ui-toc-dropdown unselectable hidden-print" data-spy="affix" style="top:17px;display:none;" null null>
        <div class="toc"><ul class="nav">
<li class=""><a href="#Deep-Learning-Image-Captioning-Project-Documentation" title="Deep Learning Image Captioning Project Documentation">Deep Learning Image Captioning Project Documentation</a><ul class="nav">
<li><a href="#Folder-Structure" title="Folder Structure">Folder Structure</a></li>
<li><a href="#Installation" title="Installation">Installation</a></li>
<li><a href="#Execution" title="Execution">Execution</a><ul class="nav">
<li><a href="#Training" title="Training">Training</a></li>
</ul>
</li>
<li><a href="#Model-Design" title="Model Design">Model Design</a></li>
<li><a href="#Architecture" title="Architecture">Architecture</a></li>
<li><a href="#Training-Hyperparameters" title="Training Hyperparameters">Training Hyperparameters</a></li>
</ul>
</li>
<li class=""><a href="#Image-Pre-Processing-for-training" title="Image Pre-Processing for training">Image Pre-Processing for training</a><ul class="nav">
<li><a href="#Vocabulary-Preprocessing" title="Vocabulary Preprocessing">Vocabulary Preprocessing</a></li>
<li><a href="#Models" title="Models">Models</a><ul class="nav">
<li><a href="#EncoderCNN-Model" title="EncoderCNN Model">EncoderCNN Model</a></li>
<li><a href="#DecoderRNN-Model" title="DecoderRNN Model">DecoderRNN Model</a></li>
<li><a href="#EncoderRNN-Model" title="EncoderRNN Model">EncoderRNN Model</a></li>
<li><a href="#Generator-Model" title="Generator Model">Generator Model</a></li>
<li><a href="#Discriminator-Model" title="Discriminator Model">Discriminator Model</a></li>
</ul>
</li>
<li><a href="#Loss-Function" title="Loss Function">Loss Function</a></li>
<li><a href="#Performance" title="Performance">Performance</a></li>
<li><a href="#GUI" title="GUI">GUI</a></li>
<li><a href="#Group-Members-Contributions" title="Group Members Contributions">Group Members Contributions</a></li>
<li class=""><a href="#WeirdestFunniest-Captions" title="Weirdest/Funniest Captions">Weirdest/Funniest Captions</a></li>
</ul>
</li>
</ul>
</div><div class="toc-menu"><a class="expand-toggle" href="#">Expand all</a><a class="back-to-top" href="#">Back to top</a><a class="go-to-bottom" href="#">Go to bottom</a></div>
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.min.js" integrity="sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha256-U5ZEeKfGNOja007MMD3YBI0A3OSZOQbeG6z2f2Y0hu8=" crossorigin="anonymous" defer></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gist-embed/2.6.0/gist-embed.min.js" integrity="sha256-KyF2D6xPIJUW5sUDSs93vWyZm+1RzIpKCexxElmxl8g=" crossorigin="anonymous" defer></script>
    <script>
        var markdown = $(".markdown-body");
        //smooth all hash trigger scrolling
        function smoothHashScroll() {
            var hashElements = $("a[href^='#']").toArray();
            for (var i = 0; i < hashElements.length; i++) {
                var element = hashElements[i];
                var $element = $(element);
                var hash = element.hash;
                if (hash) {
                    $element.on('click', function (e) {
                        // store hash
                        var hash = this.hash;
                        if ($(hash).length <= 0) return;
                        // prevent default anchor click behavior
                        e.preventDefault();
                        // animate
                        $('body, html').stop(true, true).animate({
                            scrollTop: $(hash).offset().top
                        }, 100, "linear", function () {
                            // when done, add hash to url
                            // (default click behaviour)
                            window.location.hash = hash;
                        });
                    });
                }
            }
        }

        smoothHashScroll();
        var toc = $('.ui-toc');
        var tocAffix = $('.ui-affix-toc');
        var tocDropdown = $('.ui-toc-dropdown');
        //toc
        tocDropdown.click(function (e) {
            e.stopPropagation();
        });

        var enoughForAffixToc = true;

        function generateScrollspy() {
            $(document.body).scrollspy({
                target: ''
            });
            $(document.body).scrollspy('refresh');
            if (enoughForAffixToc) {
                toc.hide();
                tocAffix.show();
            } else {
                tocAffix.hide();
                toc.show();
            }
            $(document.body).scroll();
        }

        function windowResize() {
            //toc right
            var paddingRight = parseFloat(markdown.css('padding-right'));
            var right = ($(window).width() - (markdown.offset().left + markdown.outerWidth() - paddingRight));
            toc.css('right', right + 'px');
            //affix toc left
            var newbool;
            var rightMargin = (markdown.parent().outerWidth() - markdown.outerWidth()) / 2;
            //for ipad or wider device
            if (rightMargin >= 133) {
                newbool = true;
                var affixLeftMargin = (tocAffix.outerWidth() - tocAffix.width()) / 2;
                var left = markdown.offset().left + markdown.outerWidth() - affixLeftMargin;
                tocAffix.css('left', left + 'px');
            } else {
                newbool = false;
            }
            if (newbool != enoughForAffixToc) {
                enoughForAffixToc = newbool;
                generateScrollspy();
            }
        }
        $(window).resize(function () {
            windowResize();
        });
        $(document).ready(function () {
            windowResize();
            generateScrollspy();
        });

        //remove hash
        function removeHash() {
            window.location.hash = '';
        }

        var backtotop = $('.back-to-top');
        var gotobottom = $('.go-to-bottom');

        backtotop.click(function (e) {
            e.preventDefault();
            e.stopPropagation();
            if (scrollToTop)
                scrollToTop();
            removeHash();
        });
        gotobottom.click(function (e) {
            e.preventDefault();
            e.stopPropagation();
            if (scrollToBottom)
                scrollToBottom();
            removeHash();
        });

        var toggle = $('.expand-toggle');
        var tocExpand = false;

        checkExpandToggle();
        toggle.click(function (e) {
            e.preventDefault();
            e.stopPropagation();
            tocExpand = !tocExpand;
            checkExpandToggle();
        })

        function checkExpandToggle () {
            var toc = $('.ui-toc-dropdown .toc');
            var toggle = $('.expand-toggle');
            if (!tocExpand) {
                toc.removeClass('expand');
                toggle.text('Expand all');
            } else {
                toc.addClass('expand');
                toggle.text('Collapse all');
            }
        }

        function scrollToTop() {
            $('body, html').stop(true, true).animate({
                scrollTop: 0
            }, 100, "linear");
        }

        function scrollToBottom() {
            $('body, html').stop(true, true).animate({
                scrollTop: $(document.body)[0].scrollHeight
            }, 100, "linear");
        }
    </script>
</body>

</html>
