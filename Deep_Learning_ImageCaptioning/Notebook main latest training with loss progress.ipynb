{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycocotools in ./myenv/lib/python3.7/site-packages (2.0.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycocotools in ./myenv/lib/python3.7/site-packages (2.0.0)\n",
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/414113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.56s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:37<00:00, 10970.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8856\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "from pycocotools.coco import COCO\n",
    "from vocabulary import Vocabulary\n",
    "from data_loader import get_loader\n",
    "import math\n",
    "\n",
    "\n",
    "## TODO #1: Select appropriate values for the Python variables below.\n",
    "\n",
    "batch_size = 16         # batch size\n",
    "vocab_threshold = 5        # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "\n",
    "# (Optional) TODO #2: Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "print(vocab_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        \n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.train_params = list(self.embed.parameters())\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.embed(features)\n",
    "        return features\n",
    "    \n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = embed_size,hidden_size = hidden_size,\n",
    "                            num_layers = num_layers, batch_first = True)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.train_params = list(self.parameters())\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        captions = captions[:, :-1]\n",
    "        embed = self.embedding_layer(captions)\n",
    "        embed = torch.cat((features.unsqueeze(1), embed), dim = 1)\n",
    "        lstm_outputs, _ = self.lstm(embed)\n",
    "        out = self.linear(lstm_outputs)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def sample(self, inputs, states=None, max_len=20):\n",
    "        \" accepts pre-processed image tensor (inputs) and returns predicted sentence (list of tensor ids of length max_len) \"\n",
    "        output_sentence = []\n",
    "        for i in range(max_len):\n",
    "            lstm_outputs, states = self.lstm(inputs, states)\n",
    "            lstm_outputs = lstm_outputs.squeeze(1)\n",
    "            out = self.linear(lstm_outputs)\n",
    "            last_pick = out.max(1)[1]\n",
    "            output_sentence.append(last_pick.item())\n",
    "            inputs = self.embedding_layer(last_pick).unsqueeze(1)\n",
    "        \n",
    "        return output_sentence\n",
    "    \n",
    "    def beam_sample(self, inputs, states=None, max_len=20, k=1):\n",
    "        \" accepts pre-processed image tensor (inputs) and returns predicted sentence (list of tensor ids of length max_len) \"\n",
    "        possible_seq = [(1, inputs, states)]\n",
    "        for i in range(max_len):\n",
    "            to_pick = []\n",
    "            for probs,seq,states in possible_seq:\n",
    "                inputs = self.embedding_layer(seq[-1])\n",
    "                lstm_outputs, states = self.lstm(inputs, states)\n",
    "                out = self.linear(lstm_outputs).squeeze(0)\n",
    "                sorted_out, indices = torch.sort(out, 1)\n",
    "                \n",
    "                for j in range(k):\n",
    "                    to_pick.append((probs + nn.functional.log_softmax(sorted_out[i]), inputs + [indices[i]], states) )\n",
    "                 \n",
    "            to_pick.sort(reverse=True)\n",
    "            possible_seq = to_pick[:k]\n",
    "            \n",
    "        return to_pick[0]\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, context_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        #self.embedding_layer = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(input_size = embed_size,hidden_size = hidden_size,\n",
    "                            num_layers = num_layers, batch_first = True)\n",
    "        self.linear = nn.Linear(hidden_size, context_size)\n",
    "        self.train_params = list(self.parameters())\n",
    "    \n",
    "    def forward(self, captions):\n",
    "        #captions = captions[:, :-1]\n",
    "        #embed = self.embedding_layer(captions)\n",
    "        lstm_outputs, _ = self.lstm(captions)\n",
    "        out = self.linear(lstm_outputs[:,-1,:].squeeze(1))\n",
    "        return out\n",
    "\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, context_size, num_layers=1):\n",
    "        super(Generator, self).__init__()\n",
    "        self.cnn = EncoderCNN(context_size)\n",
    "        self.rnn = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers=num_layers)\n",
    "        self.train_params = self.cnn.train_params + self.rnn.train_params \n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        features = self.cnn(images)\n",
    "        output = self.rnn(features, captions)\n",
    "        return output, features\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, context_size, num_layers=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.embed = nn.Linear(vocab_size, embed_size)\n",
    "        self.rnn = EncoderRNN(embed_size, hidden_size, vocab_size, context_size, num_layers=num_layers)\n",
    "        #self.crit = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "        self.train_params = list(self.embed.parameters()) + self.rnn.train_params\n",
    "        \n",
    "\n",
    "    def forward(self, image_feat, captions):\n",
    "        bs = image_feat.size(0)\n",
    "        embed = self.embed(captions)\n",
    "        cap_feat = self.rnn(embed)\n",
    "        return cap_feat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 300\n",
    "embed_size = 300           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 3             # number of training epochs\n",
    "save_every = 1000             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "generator = Generator(embed_size, hidden_size, vocab_size, context_size, num_layers=2)\n",
    "discriminator = Discriminator(embed_size, hidden_size, vocab_size, context_size, num_layers=2)\n",
    "\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "\n",
    "# Define the loss function.\n",
    "cosineSim = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "pos_weight = torch.Tensor([batch_size-1])\n",
    "criterion_A = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "criterion_B = nn.BCELoss(pos_weight).cuda()\n",
    "\n",
    "\n",
    "# TODO #3: Specify the learnable parameters of the model.\n",
    "params = generator.train_params + discriminator.train_params\n",
    "\n",
    "# TODO #4: Define the optimizer.\n",
    "optimizer = torch.optim.Adam(params = params, lr = 0.0001)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [3/25883], Loss: (4.5959, 3.6289, 0.9670), Perplexity: 99.07645"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/test/Desktop/myenv/lib/python3.7/site-packages/ipykernel_launcher.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/25883], Loss: (4.9586, 3.9503, 1.0082), Perplexity: 142.3885\n",
      "Epoch [1/3], Step [200/25883], Loss: (4.9526, 3.9594, 0.9932), Perplexity: 141.5416\n",
      "Epoch [1/3], Step [300/25883], Loss: (4.8839, 3.8979, 0.9860), Perplexity: 132.1391\n",
      "Epoch [1/3], Step [400/25883], Loss: (5.3078, 4.2650, 1.0428), Perplexity: 201.9135\n",
      "Epoch [1/3], Step [500/25883], Loss: (4.6696, 3.6394, 1.0302), Perplexity: 106.6579\n",
      "Epoch [1/3], Step [600/25883], Loss: (5.1971, 4.1472, 1.0499), Perplexity: 180.7523\n",
      "Epoch [1/3], Step [700/25883], Loss: (4.6846, 3.6907, 0.9939), Perplexity: 108.2631\n",
      "Epoch [1/3], Step [800/25883], Loss: (4.6077, 3.6290, 0.9787), Perplexity: 100.2547\n",
      "Epoch [1/3], Step [900/25883], Loss: (4.3824, 3.3477, 1.0347), Perplexity: 80.02904\n",
      "Epoch [1/3], Step [1000/25883], Loss: (4.8732, 3.8323, 1.0409), Perplexity: 130.7389\n",
      "Epoch [1/3], Step [1100/25883], Loss: (4.9259, 3.9434, 0.9825), Perplexity: 137.8186\n",
      "Epoch [1/3], Step [1200/25883], Loss: (4.2270, 3.2264, 1.0006), Perplexity: 68.51069\n",
      "Epoch [1/3], Step [1300/25883], Loss: (4.5754, 3.5889, 0.9864), Perplexity: 97.06284\n",
      "Epoch [1/3], Step [1400/25883], Loss: (4.6641, 3.6775, 0.9866), Perplexity: 106.0664\n",
      "Epoch [1/3], Step [1500/25883], Loss: (4.9509, 3.9280, 1.0229), Perplexity: 141.3025\n",
      "Epoch [1/3], Step [1600/25883], Loss: (4.6541, 3.6218, 1.0324), Perplexity: 105.0178\n",
      "Epoch [1/3], Step [1700/25883], Loss: (4.2987, 3.3080, 0.9907), Perplexity: 73.60137\n",
      "Epoch [1/3], Step [1800/25883], Loss: (4.7851, 3.7868, 0.9983), Perplexity: 119.7125\n",
      "Epoch [1/3], Step [1900/25883], Loss: (4.4031, 3.3593, 1.0438), Perplexity: 81.70156\n",
      "Epoch [1/3], Step [2000/25883], Loss: (4.3982, 3.3822, 1.0160), Perplexity: 81.30748\n",
      "Epoch [1/3], Step [2100/25883], Loss: (4.4057, 3.3223, 1.0834), Perplexity: 81.91846\n",
      "Epoch [1/3], Step [2200/25883], Loss: (4.3871, 3.3527, 1.0344), Perplexity: 80.40494\n",
      "Epoch [1/3], Step [2300/25883], Loss: (5.2678, 4.2844, 0.9835), Perplexity: 193.9956\n",
      "Epoch [1/3], Step [2400/25883], Loss: (4.5675, 3.6085, 0.9590), Perplexity: 96.30719\n",
      "Epoch [1/3], Step [2500/25883], Loss: (4.7807, 3.7649, 1.0158), Perplexity: 119.1936\n",
      "Epoch [1/3], Step [2600/25883], Loss: (5.2814, 4.1020, 1.1794), Perplexity: 196.6397\n",
      "Epoch [1/3], Step [2700/25883], Loss: (4.3574, 3.4030, 0.9544), Perplexity: 78.05316\n",
      "Epoch [1/3], Step [2800/25883], Loss: (4.8314, 3.7228, 1.1087), Perplexity: 125.3921\n",
      "Epoch [1/3], Step [2900/25883], Loss: (4.4880, 3.4733, 1.0147), Perplexity: 88.93948\n",
      "Epoch [1/3], Step [3000/25883], Loss: (3.9870, 3.0943, 0.8927), Perplexity: 53.89286\n",
      "Epoch [1/3], Step [3100/25883], Loss: (4.7496, 3.7332, 1.0164), Perplexity: 115.5409\n",
      "Epoch [1/3], Step [3200/25883], Loss: (3.7609, 2.7930, 0.9679), Perplexity: 42.98772\n",
      "Epoch [1/3], Step [3300/25883], Loss: (4.2944, 3.4310, 0.8634), Perplexity: 73.28571\n",
      "Epoch [1/3], Step [3400/25883], Loss: (4.2354, 3.2458, 0.9896), Perplexity: 69.09246\n",
      "Epoch [1/3], Step [3500/25883], Loss: (4.0957, 3.1185, 0.9773), Perplexity: 60.08335\n",
      "Epoch [1/3], Step [3600/25883], Loss: (4.8081, 3.8300, 0.9781), Perplexity: 122.5039\n",
      "Epoch [1/3], Step [3700/25883], Loss: (5.0632, 4.0089, 1.0543), Perplexity: 158.1022\n",
      "Epoch [1/3], Step [3800/25883], Loss: (5.1785, 4.0133, 1.1652), Perplexity: 177.4119\n",
      "Epoch [1/3], Step [3900/25883], Loss: (4.2536, 3.2392, 1.0144), Perplexity: 70.35633\n",
      "Epoch [1/3], Step [4000/25883], Loss: (4.3080, 3.2342, 1.0738), Perplexity: 74.29063\n",
      "Epoch [1/3], Step [4100/25883], Loss: (4.2787, 3.3423, 0.9363), Perplexity: 72.14352\n",
      "Epoch [1/3], Step [4200/25883], Loss: (3.9080, 2.8381, 1.0700), Perplexity: 49.80124\n",
      "Epoch [1/3], Step [4300/25883], Loss: (4.2914, 3.0112, 1.2802), Perplexity: 73.07031\n",
      "Epoch [1/3], Step [4400/25883], Loss: (4.0289, 3.1204, 0.9085), Perplexity: 56.19785\n",
      "Epoch [1/3], Step [4500/25883], Loss: (4.0104, 3.0591, 0.9513), Perplexity: 55.16962\n",
      "Epoch [1/3], Step [4600/25883], Loss: (4.2529, 3.3124, 0.9405), Perplexity: 70.31192\n",
      "Epoch [1/3], Step [4700/25883], Loss: (3.5487, 2.6708, 0.8779), Perplexity: 34.76857\n",
      "Epoch [1/3], Step [4800/25883], Loss: (4.0481, 3.0108, 1.0373), Perplexity: 57.28735\n",
      "Epoch [1/3], Step [4900/25883], Loss: (4.2267, 2.8089, 1.4178), Perplexity: 68.49166\n",
      "Epoch [1/3], Step [5000/25883], Loss: (3.9789, 3.3783, 0.6007), Perplexity: 53.46083\n",
      "Epoch [1/3], Step [5100/25883], Loss: (3.9505, 2.9637, 0.9868), Perplexity: 51.96247\n",
      "Epoch [1/3], Step [5200/25883], Loss: (4.1127, 3.4130, 0.6998), Perplexity: 61.11342\n",
      "Epoch [1/3], Step [5300/25883], Loss: (4.2525, 3.4181, 0.8345), Perplexity: 70.28293\n",
      "Epoch [1/3], Step [5400/25883], Loss: (4.2986, 3.7488, 0.5498), Perplexity: 73.59551\n",
      "Epoch [1/3], Step [5500/25883], Loss: (3.6682, 2.9957, 0.6725), Perplexity: 39.18263\n",
      "Epoch [1/3], Step [5600/25883], Loss: (3.7671, 3.0169, 0.7502), Perplexity: 43.25440\n",
      "Epoch [1/3], Step [5700/25883], Loss: (4.4053, 3.5908, 0.8144), Perplexity: 81.88238\n",
      "Epoch [1/3], Step [5800/25883], Loss: (4.3163, 3.2506, 1.0657), Perplexity: 74.91228\n",
      "Epoch [1/3], Step [5900/25883], Loss: (4.4231, 3.4979, 0.9252), Perplexity: 83.35280\n",
      "Epoch [1/3], Step [6000/25883], Loss: (3.9947, 3.0305, 0.9642), Perplexity: 54.30946\n",
      "Epoch [1/3], Step [6100/25883], Loss: (3.9287, 2.9415, 0.9872), Perplexity: 50.84250\n",
      "Epoch [1/3], Step [6200/25883], Loss: (4.2361, 3.2040, 1.0322), Perplexity: 69.13864\n",
      "Epoch [1/3], Step [6300/25883], Loss: (4.0215, 2.8173, 1.2042), Perplexity: 55.78271\n",
      "Epoch [1/3], Step [6400/25883], Loss: (3.9514, 3.1014, 0.8500), Perplexity: 52.00901\n",
      "Epoch [1/3], Step [6500/25883], Loss: (4.0714, 3.1341, 0.9373), Perplexity: 58.63882\n",
      "Epoch [1/3], Step [6600/25883], Loss: (3.6344, 3.0861, 0.5483), Perplexity: 37.87906\n",
      "Epoch [1/3], Step [6700/25883], Loss: (4.6179, 3.5489, 1.0690), Perplexity: 101.2784\n",
      "Epoch [1/3], Step [6800/25883], Loss: (3.6700, 2.7195, 0.9505), Perplexity: 39.25009\n",
      "Epoch [1/3], Step [6900/25883], Loss: (5.6709, 4.7828, 0.8882), Perplexity: 290.3081\n",
      "Epoch [1/3], Step [7000/25883], Loss: (3.5898, 2.8450, 0.7448), Perplexity: 36.22742\n",
      "Epoch [1/3], Step [7100/25883], Loss: (3.5268, 2.5795, 0.9473), Perplexity: 34.01477\n",
      "Epoch [1/3], Step [7200/25883], Loss: (4.4465, 3.2026, 1.2439), Perplexity: 85.32824\n",
      "Epoch [1/3], Step [7300/25883], Loss: (4.1517, 3.3890, 0.7626), Perplexity: 63.53890\n",
      "Epoch [1/3], Step [7400/25883], Loss: (3.7864, 2.9224, 0.8640), Perplexity: 44.09902\n",
      "Epoch [1/3], Step [7500/25883], Loss: (3.6143, 3.1424, 0.4719), Perplexity: 37.12502\n",
      "Epoch [1/3], Step [7600/25883], Loss: (2.8900, 2.4760, 0.4140), Perplexity: 17.99318\n",
      "Epoch [1/3], Step [7700/25883], Loss: (3.8025, 3.3470, 0.4555), Perplexity: 44.81174\n",
      "Epoch [1/3], Step [7800/25883], Loss: (3.0886, 2.8000, 0.2885), Perplexity: 21.94615\n",
      "Epoch [1/3], Step [7900/25883], Loss: (3.6544, 2.7416, 0.9128), Perplexity: 38.64248\n",
      "Epoch [1/3], Step [8000/25883], Loss: (3.0732, 2.7688, 0.3044), Perplexity: 21.61196\n",
      "Epoch [1/3], Step [8100/25883], Loss: (4.3122, 2.9629, 1.3493), Perplexity: 74.60804\n",
      "Epoch [1/3], Step [8200/25883], Loss: (3.1051, 2.6643, 0.4408), Perplexity: 22.31188\n",
      "Epoch [1/3], Step [8300/25883], Loss: (4.1323, 3.3364, 0.7959), Perplexity: 62.3227\n",
      "Epoch [1/3], Step [8400/25883], Loss: (3.8116, 3.4558, 0.3558), Perplexity: 45.2215\n",
      "Epoch [1/3], Step [8500/25883], Loss: (2.9771, 2.7935, 0.1835), Perplexity: 19.63002\n",
      "Epoch [1/3], Step [8600/25883], Loss: (3.5106, 3.0176, 0.4930), Perplexity: 33.4680\n",
      "Epoch [1/3], Step [8700/25883], Loss: (3.6677, 3.2314, 0.4363), Perplexity: 39.16333\n",
      "Epoch [1/3], Step [8800/25883], Loss: (3.2876, 2.9565, 0.3310), Perplexity: 26.77766\n",
      "Epoch [1/3], Step [8900/25883], Loss: (3.7298, 3.1616, 0.5682), Perplexity: 41.67045\n",
      "Epoch [1/3], Step [9000/25883], Loss: (3.4122, 3.0150, 0.3971), Perplexity: 30.33124\n",
      "Epoch [1/3], Step [9100/25883], Loss: (3.6965, 3.2649, 0.4316), Perplexity: 40.30427\n",
      "Epoch [1/3], Step [9200/25883], Loss: (3.7125, 3.2604, 0.4521), Perplexity: 40.95503\n",
      "Epoch [1/3], Step [9300/25883], Loss: (3.1835, 2.6713, 0.5121), Perplexity: 24.12993\n",
      "Epoch [1/3], Step [9400/25883], Loss: (3.6963, 3.2107, 0.4856), Perplexity: 40.29873\n",
      "Epoch [1/3], Step [9500/25883], Loss: (3.2757, 2.5014, 0.7744), Perplexity: 26.46274\n",
      "Epoch [1/3], Step [9600/25883], Loss: (3.7251, 3.1002, 0.6249), Perplexity: 41.47658\n",
      "Epoch [1/3], Step [9700/25883], Loss: (3.6392, 2.6218, 1.0174), Perplexity: 38.05990\n",
      "Epoch [1/3], Step [9800/25883], Loss: (3.2474, 2.9824, 0.2650), Perplexity: 25.72295\n",
      "Epoch [1/3], Step [9900/25883], Loss: (3.8909, 3.0376, 0.8533), Perplexity: 48.95495\n",
      "Epoch [1/3], Step [10000/25883], Loss: (2.8363, 2.5823, 0.2540), Perplexity: 17.0527\n",
      "Epoch [1/3], Step [10100/25883], Loss: (4.8157, 3.7835, 1.0322), Perplexity: 123.4358\n",
      "Epoch [1/3], Step [10200/25883], Loss: (3.1697, 2.8321, 0.3376), Perplexity: 23.8012\n",
      "Epoch [1/3], Step [10300/25883], Loss: (4.1258, 3.8409, 0.2849), Perplexity: 61.91812\n",
      "Epoch [1/3], Step [10400/25883], Loss: (4.0870, 3.6033, 0.4837), Perplexity: 59.5611\n",
      "Epoch [1/3], Step [10500/25883], Loss: (3.1528, 2.8836, 0.2692), Perplexity: 23.40168\n",
      "Epoch [1/3], Step [10600/25883], Loss: (3.8000, 3.4928, 0.3073), Perplexity: 44.7018\n",
      "Epoch [1/3], Step [10700/25883], Loss: (3.4910, 3.0838, 0.4072), Perplexity: 32.81750\n",
      "Epoch [1/3], Step [10800/25883], Loss: (3.5850, 3.3697, 0.2152), Perplexity: 36.0531\n",
      "Epoch [1/3], Step [10900/25883], Loss: (3.3418, 2.9652, 0.3765), Perplexity: 28.26864\n",
      "Epoch [1/3], Step [11000/25883], Loss: (3.7723, 3.1078, 0.6645), Perplexity: 43.4806\n",
      "Epoch [1/3], Step [11100/25883], Loss: (3.2735, 2.9396, 0.3339), Perplexity: 26.4042\n",
      "Epoch [1/3], Step [11200/25883], Loss: (3.6233, 3.2503, 0.3730), Perplexity: 37.46170\n",
      "Epoch [1/3], Step [11300/25883], Loss: (3.6833, 3.3614, 0.3219), Perplexity: 39.7786\n",
      "Epoch [1/3], Step [11400/25883], Loss: (3.7751, 3.3102, 0.4650), Perplexity: 43.60380\n",
      "Epoch [1/3], Step [11500/25883], Loss: (3.0868, 2.7220, 0.3649), Perplexity: 21.90778\n",
      "Epoch [1/3], Step [11600/25883], Loss: (3.2469, 2.8351, 0.4117), Perplexity: 25.70983\n",
      "Epoch [1/3], Step [11700/25883], Loss: (3.4887, 2.9157, 0.5730), Perplexity: 32.74370\n",
      "Epoch [1/3], Step [11800/25883], Loss: (3.1980, 2.8145, 0.3835), Perplexity: 24.48340\n",
      "Epoch [1/3], Step [11900/25883], Loss: (3.2022, 2.7133, 0.4890), Perplexity: 24.58769\n",
      "Epoch [1/3], Step [12000/25883], Loss: (3.7224, 2.7324, 0.9900), Perplexity: 41.3624\n",
      "Epoch [1/3], Step [12100/25883], Loss: (3.6443, 3.3032, 0.3411), Perplexity: 38.2556\n",
      "Epoch [1/3], Step [12200/25883], Loss: (3.3946, 2.9451, 0.4495), Perplexity: 29.8014\n",
      "Epoch [1/3], Step [12300/25883], Loss: (3.8594, 3.2352, 0.6241), Perplexity: 47.43495\n",
      "Epoch [1/3], Step [12400/25883], Loss: (2.9333, 2.5479, 0.3854), Perplexity: 18.78956\n",
      "Epoch [1/3], Step [12500/25883], Loss: (2.7628, 2.5686, 0.1941), Perplexity: 15.84370\n",
      "Epoch [1/3], Step [12600/25883], Loss: (3.0820, 2.8607, 0.2212), Perplexity: 21.80110\n",
      "Epoch [1/3], Step [12700/25883], Loss: (2.9038, 2.5617, 0.3422), Perplexity: 18.2438\n",
      "Epoch [1/3], Step [12800/25883], Loss: (2.7978, 2.3924, 0.4054), Perplexity: 16.40895\n",
      "Epoch [1/3], Step [12900/25883], Loss: (3.0056, 2.6606, 0.3450), Perplexity: 20.1980\n",
      "Epoch [1/3], Step [13000/25883], Loss: (3.8918, 3.4874, 0.4044), Perplexity: 48.99902\n",
      "Epoch [1/3], Step [13100/25883], Loss: (2.9405, 2.5983, 0.3422), Perplexity: 18.92524\n",
      "Epoch [1/3], Step [13200/25883], Loss: (2.8250, 2.5355, 0.2895), Perplexity: 16.8614\n",
      "Epoch [1/3], Step [13300/25883], Loss: (3.0629, 2.7929, 0.2701), Perplexity: 21.3905\n",
      "Epoch [1/3], Step [13400/25883], Loss: (3.7405, 3.2809, 0.4596), Perplexity: 42.11875\n",
      "Epoch [1/3], Step [13500/25883], Loss: (2.8782, 2.4487, 0.4295), Perplexity: 17.78193\n",
      "Epoch [1/3], Step [13600/25883], Loss: (3.1106, 2.4627, 0.6478), Perplexity: 22.4334\n",
      "Epoch [1/3], Step [13700/25883], Loss: (2.8372, 2.6318, 0.2054), Perplexity: 17.0683\n",
      "Epoch [1/3], Step [13800/25883], Loss: (3.0085, 2.5888, 0.4198), Perplexity: 20.2578\n",
      "Epoch [1/3], Step [13900/25883], Loss: (2.9613, 2.8008, 0.1605), Perplexity: 19.32406\n",
      "Epoch [1/3], Step [14000/25883], Loss: (4.2983, 2.7890, 1.5093), Perplexity: 73.57750\n",
      "Epoch [1/3], Step [14100/25883], Loss: (3.4114, 2.8570, 0.5544), Perplexity: 30.30628\n",
      "Epoch [1/3], Step [14200/25883], Loss: (3.1692, 2.7012, 0.4681), Perplexity: 23.78857\n",
      "Epoch [1/3], Step [14300/25883], Loss: (2.4044, 2.2659, 0.1385), Perplexity: 11.0722\n",
      "Epoch [1/3], Step [14400/25883], Loss: (3.1640, 2.9300, 0.2340), Perplexity: 23.6647\n",
      "Epoch [1/3], Step [14500/25883], Loss: (3.3815, 3.0884, 0.2931), Perplexity: 29.4143\n",
      "Epoch [1/3], Step [14600/25883], Loss: (3.4888, 3.0436, 0.4452), Perplexity: 32.74804\n",
      "Epoch [1/3], Step [14700/25883], Loss: (3.2363, 2.9915, 0.2448), Perplexity: 25.43884\n",
      "Epoch [1/3], Step [14800/25883], Loss: (3.5655, 3.0904, 0.4751), Perplexity: 35.35610\n",
      "Epoch [1/3], Step [14900/25883], Loss: (3.2821, 2.6169, 0.6652), Perplexity: 26.63166\n",
      "Epoch [1/3], Step [15000/25883], Loss: (2.8309, 2.4466, 0.3843), Perplexity: 16.9605\n",
      "Epoch [1/3], Step [15100/25883], Loss: (3.3852, 3.1113, 0.2739), Perplexity: 29.5241\n",
      "Epoch [1/3], Step [15200/25883], Loss: (2.7351, 2.4439, 0.2912), Perplexity: 15.4115\n",
      "Epoch [1/3], Step [15300/25883], Loss: (3.2598, 2.4698, 0.7900), Perplexity: 26.04407\n",
      "Epoch [1/3], Step [15400/25883], Loss: (3.5021, 3.0373, 0.4649), Perplexity: 33.1867\n",
      "Epoch [1/3], Step [15500/25883], Loss: (3.2229, 2.2260, 0.9969), Perplexity: 25.10172\n",
      "Epoch [1/3], Step [15600/25883], Loss: (2.8814, 2.6985, 0.1829), Perplexity: 17.8385\n",
      "Epoch [1/3], Step [15700/25883], Loss: (3.5080, 2.8709, 0.6370), Perplexity: 33.3811\n",
      "Epoch [1/3], Step [15800/25883], Loss: (2.7539, 2.3388, 0.4151), Perplexity: 15.7034\n",
      "Epoch [1/3], Step [15900/25883], Loss: (2.8368, 2.6235, 0.2133), Perplexity: 17.06138\n",
      "Epoch [1/3], Step [16000/25883], Loss: (2.8557, 2.6120, 0.2437), Perplexity: 17.38687\n",
      "Epoch [1/3], Step [16100/25883], Loss: (2.9939, 2.7718, 0.2221), Perplexity: 19.9624\n",
      "Epoch [1/3], Step [16200/25883], Loss: (3.7958, 2.7292, 1.0666), Perplexity: 44.51285\n",
      "Epoch [1/3], Step [16300/25883], Loss: (2.7469, 2.5492, 0.1977), Perplexity: 15.59421\n",
      "Epoch [1/3], Step [16400/25883], Loss: (3.6824, 3.0757, 0.6066), Perplexity: 39.74014\n",
      "Epoch [1/3], Step [16500/25883], Loss: (2.9645, 2.5332, 0.4312), Perplexity: 19.3843\n",
      "Epoch [1/3], Step [16600/25883], Loss: (2.9985, 2.5941, 0.4045), Perplexity: 20.0558\n",
      "Epoch [1/3], Step [16700/25883], Loss: (3.1317, 2.8434, 0.2884), Perplexity: 22.91371\n",
      "Epoch [1/3], Step [16800/25883], Loss: (2.9028, 2.6326, 0.2702), Perplexity: 18.22512\n",
      "Epoch [1/3], Step [16900/25883], Loss: (3.7004, 3.0405, 0.6599), Perplexity: 40.4628\n",
      "Epoch [1/3], Step [17000/25883], Loss: (3.0103, 2.6092, 0.4011), Perplexity: 20.29326\n",
      "Epoch [1/3], Step [17100/25883], Loss: (3.9351, 3.0729, 0.8622), Perplexity: 51.1685\n",
      "Epoch [1/3], Step [17200/25883], Loss: (4.0499, 3.6315, 0.4184), Perplexity: 57.3907\n",
      "Epoch [1/3], Step [17300/25883], Loss: (2.6699, 2.2810, 0.3889), Perplexity: 14.4384\n",
      "Epoch [1/3], Step [17400/25883], Loss: (3.9522, 2.8177, 1.1345), Perplexity: 52.04896\n",
      "Epoch [1/3], Step [17500/25883], Loss: (3.0143, 2.6640, 0.3503), Perplexity: 20.3749\n",
      "Epoch [1/3], Step [17600/25883], Loss: (3.0459, 2.6958, 0.3501), Perplexity: 21.02833\n",
      "Epoch [1/3], Step [17700/25883], Loss: (2.8406, 2.5186, 0.3219), Perplexity: 17.1253\n",
      "Epoch [1/3], Step [17800/25883], Loss: (3.4477, 3.0140, 0.4337), Perplexity: 31.4291\n",
      "Epoch [1/3], Step [17900/25883], Loss: (3.1918, 2.9539, 0.2379), Perplexity: 24.3319\n",
      "Epoch [1/3], Step [18000/25883], Loss: (2.6452, 2.3904, 0.2549), Perplexity: 14.0869\n",
      "Epoch [1/3], Step [18100/25883], Loss: (3.0412, 2.8301, 0.2111), Perplexity: 20.9297\n",
      "Epoch [1/3], Step [18200/25883], Loss: (3.3888, 2.7653, 0.6235), Perplexity: 29.6301\n",
      "Epoch [1/3], Step [18300/25883], Loss: (3.4567, 3.1415, 0.3152), Perplexity: 31.7113\n",
      "Epoch [1/3], Step [18400/25883], Loss: (3.2792, 2.7242, 0.5549), Perplexity: 26.55334\n",
      "Epoch [1/3], Step [18500/25883], Loss: (2.8240, 2.5606, 0.2634), Perplexity: 16.8438\n",
      "Epoch [1/3], Step [18600/25883], Loss: (2.5135, 2.2833, 0.2302), Perplexity: 12.3479\n",
      "Epoch [1/3], Step [18700/25883], Loss: (3.0376, 2.6470, 0.3906), Perplexity: 20.85485\n",
      "Epoch [1/3], Step [18800/25883], Loss: (3.5573, 2.8565, 0.7008), Perplexity: 35.06776\n",
      "Epoch [1/3], Step [18900/25883], Loss: (2.9643, 2.7023, 0.2621), Perplexity: 19.3818\n",
      "Epoch [1/3], Step [19000/25883], Loss: (3.2976, 2.9267, 0.3708), Perplexity: 27.0474\n",
      "Epoch [1/3], Step [19100/25883], Loss: (3.3432, 2.7971, 0.5460), Perplexity: 28.3090\n",
      "Epoch [1/3], Step [19200/25883], Loss: (3.7344, 3.4178, 0.3167), Perplexity: 41.8648\n",
      "Epoch [1/3], Step [19300/25883], Loss: (3.3386, 2.8835, 0.4551), Perplexity: 28.1801\n",
      "Epoch [1/3], Step [19400/25883], Loss: (3.0689, 2.6496, 0.4194), Perplexity: 21.5193\n",
      "Epoch [1/3], Step [19500/25883], Loss: (3.2575, 2.9042, 0.3533), Perplexity: 25.9836\n",
      "Epoch [1/3], Step [19600/25883], Loss: (2.7889, 2.4976, 0.2912), Perplexity: 16.26288\n",
      "Epoch [1/3], Step [19700/25883], Loss: (2.8680, 2.6307, 0.2374), Perplexity: 17.6023\n",
      "Epoch [1/3], Step [19800/25883], Loss: (5.2071, 4.8336, 0.3735), Perplexity: 182.5584\n",
      "Epoch [1/3], Step [19900/25883], Loss: (2.8807, 2.3907, 0.4900), Perplexity: 17.8266\n",
      "Epoch [1/3], Step [20000/25883], Loss: (3.0166, 2.7795, 0.2371), Perplexity: 20.4214\n",
      "Epoch [1/3], Step [20100/25883], Loss: (3.7852, 3.4054, 0.3799), Perplexity: 44.0463\n",
      "Epoch [1/3], Step [20200/25883], Loss: (2.9470, 2.7007, 0.2463), Perplexity: 19.0485\n",
      "Epoch [1/3], Step [20300/25883], Loss: (2.7191, 2.4909, 0.2282), Perplexity: 15.16658\n",
      "Epoch [1/3], Step [20400/25883], Loss: (2.9714, 2.6980, 0.2734), Perplexity: 19.5190\n",
      "Epoch [1/3], Step [20500/25883], Loss: (3.1254, 2.8628, 0.2626), Perplexity: 22.76839\n",
      "Epoch [1/3], Step [20600/25883], Loss: (2.5030, 2.3090, 0.1940), Perplexity: 12.2186\n",
      "Epoch [1/3], Step [20700/25883], Loss: (2.9240, 2.5840, 0.3400), Perplexity: 18.6151\n",
      "Epoch [1/3], Step [20800/25883], Loss: (3.4046, 2.6595, 0.7451), Perplexity: 30.1024\n",
      "Epoch [1/3], Step [20900/25883], Loss: (3.3959, 2.9773, 0.4185), Perplexity: 29.8407\n",
      "Epoch [1/3], Step [21000/25883], Loss: (2.6121, 2.2488, 0.3633), Perplexity: 13.6273\n",
      "Epoch [1/3], Step [21100/25883], Loss: (3.1216, 2.9382, 0.1834), Perplexity: 22.6825\n",
      "Epoch [1/3], Step [21200/25883], Loss: (3.4010, 3.2196, 0.1814), Perplexity: 29.9946\n",
      "Epoch [1/3], Step [21300/25883], Loss: (3.1998, 2.3880, 0.8118), Perplexity: 24.5277\n",
      "Epoch [1/3], Step [21400/25883], Loss: (3.5117, 2.9692, 0.5425), Perplexity: 33.5043\n",
      "Epoch [1/3], Step [21500/25883], Loss: (2.7463, 2.5449, 0.2015), Perplexity: 15.5855\n",
      "Epoch [1/3], Step [21600/25883], Loss: (3.3067, 3.1127, 0.1940), Perplexity: 27.2941\n",
      "Epoch [1/3], Step [21700/25883], Loss: (2.9637, 2.6951, 0.2686), Perplexity: 19.3695\n",
      "Epoch [1/3], Step [21800/25883], Loss: (2.9450, 2.7870, 0.1580), Perplexity: 19.01080\n",
      "Epoch [1/3], Step [21900/25883], Loss: (3.9582, 3.3560, 0.6022), Perplexity: 52.36243\n",
      "Epoch [1/3], Step [22000/25883], Loss: (2.8328, 2.6384, 0.1945), Perplexity: 16.99370\n",
      "Epoch [1/3], Step [22100/25883], Loss: (2.9824, 2.8326, 0.1498), Perplexity: 19.7349\n",
      "Epoch [1/3], Step [22200/25883], Loss: (3.0876, 2.0977, 0.9898), Perplexity: 21.9236\n",
      "Epoch [1/3], Step [22300/25883], Loss: (2.8279, 2.4199, 0.4080), Perplexity: 16.9097\n",
      "Epoch [1/3], Step [22400/25883], Loss: (3.3727, 3.0914, 0.2813), Perplexity: 29.1585\n",
      "Epoch [1/3], Step [22500/25883], Loss: (3.0784, 2.7826, 0.2957), Perplexity: 21.7232\n",
      "Epoch [1/3], Step [22600/25883], Loss: (2.7522, 2.4491, 0.3031), Perplexity: 15.67671\n",
      "Epoch [1/3], Step [22700/25883], Loss: (3.0931, 2.7116, 0.3815), Perplexity: 22.04542\n",
      "Epoch [1/3], Step [22800/25883], Loss: (3.1730, 2.5013, 0.6716), Perplexity: 23.87819\n",
      "Epoch [1/3], Step [22900/25883], Loss: (3.1906, 2.8972, 0.2934), Perplexity: 24.3037\n",
      "Epoch [1/3], Step [23000/25883], Loss: (2.8686, 2.5143, 0.3543), Perplexity: 17.6132\n",
      "Epoch [1/3], Step [23100/25883], Loss: (2.2438, 2.0373, 0.2066), Perplexity: 9.42930\n",
      "Epoch [1/3], Step [23200/25883], Loss: (3.4970, 3.1834, 0.3136), Perplexity: 33.0146\n",
      "Epoch [1/3], Step [23300/25883], Loss: (3.0783, 2.7761, 0.3022), Perplexity: 21.7207\n",
      "Epoch [1/3], Step [23400/25883], Loss: (2.9848, 2.6246, 0.3602), Perplexity: 19.7834\n",
      "Epoch [1/3], Step [23500/25883], Loss: (2.8453, 2.5267, 0.3185), Perplexity: 17.2062\n",
      "Epoch [1/3], Step [23600/25883], Loss: (3.2416, 2.7763, 0.4654), Perplexity: 25.5758\n",
      "Epoch [1/3], Step [23700/25883], Loss: (2.5325, 2.0315, 0.5010), Perplexity: 12.5848\n",
      "Epoch [1/3], Step [23800/25883], Loss: (3.2156, 2.9714, 0.2442), Perplexity: 24.9183\n",
      "Epoch [1/3], Step [23900/25883], Loss: (2.4517, 2.3010, 0.1508), Perplexity: 11.6086\n",
      "Epoch [1/3], Step [24000/25883], Loss: (2.8351, 2.6608, 0.1743), Perplexity: 17.0314\n",
      "Epoch [1/3], Step [24100/25883], Loss: (3.4494, 3.1491, 0.3003), Perplexity: 31.4817\n",
      "Epoch [1/3], Step [24200/25883], Loss: (2.6734, 2.2193, 0.4540), Perplexity: 14.4888\n",
      "Epoch [1/3], Step [24300/25883], Loss: (3.2094, 2.8927, 0.3167), Perplexity: 24.7632\n",
      "Epoch [1/3], Step [24400/25883], Loss: (2.6626, 2.2205, 0.4421), Perplexity: 14.3335\n",
      "Epoch [1/3], Step [24500/25883], Loss: (2.7121, 2.4234, 0.2887), Perplexity: 15.06116\n",
      "Epoch [1/3], Step [24600/25883], Loss: (3.0479, 2.5444, 0.5035), Perplexity: 21.0710\n",
      "Epoch [1/3], Step [24700/25883], Loss: (3.2274, 2.4974, 0.7301), Perplexity: 25.2150\n",
      "Epoch [1/3], Step [24800/25883], Loss: (2.8833, 2.5207, 0.3626), Perplexity: 17.8732\n",
      "Epoch [1/3], Step [24900/25883], Loss: (2.9952, 2.6312, 0.3640), Perplexity: 19.9888\n",
      "Epoch [1/3], Step [25000/25883], Loss: (3.0821, 2.5980, 0.4841), Perplexity: 21.8039\n",
      "Epoch [1/3], Step [25100/25883], Loss: (2.7198, 2.5267, 0.1930), Perplexity: 15.1767\n",
      "Epoch [1/3], Step [25200/25883], Loss: (2.9120, 2.6831, 0.2289), Perplexity: 18.3935\n",
      "Epoch [1/3], Step [25300/25883], Loss: (2.8989, 2.7360, 0.1629), Perplexity: 18.1540\n",
      "Epoch [1/3], Step [25400/25883], Loss: (2.7382, 2.5482, 0.1900), Perplexity: 15.45936\n",
      "Epoch [1/3], Step [25500/25883], Loss: (3.1041, 2.7367, 0.3674), Perplexity: 22.2900\n",
      "Epoch [1/3], Step [25600/25883], Loss: (2.9953, 2.5815, 0.4138), Perplexity: 19.9921\n",
      "Epoch [1/3], Step [25700/25883], Loss: (2.6102, 2.4706, 0.1396), Perplexity: 13.6014\n",
      "Epoch [1/3], Step [25800/25883], Loss: (3.2139, 2.6925, 0.5214), Perplexity: 24.8747\n",
      "Epoch [2/3], Step [100/25883], Loss: (3.3310, 2.7244, 0.6066), Perplexity: 27.965346\n",
      "Epoch [2/3], Step [200/25883], Loss: (2.9390, 2.7266, 0.2124), Perplexity: 18.8974\n",
      "Epoch [2/3], Step [300/25883], Loss: (2.7463, 2.4121, 0.3342), Perplexity: 15.58522\n",
      "Epoch [2/3], Step [400/25883], Loss: (3.9411, 2.5932, 1.3480), Perplexity: 51.4765\n",
      "Epoch [2/3], Step [500/25883], Loss: (2.6865, 2.4594, 0.2271), Perplexity: 14.6800\n",
      "Epoch [2/3], Step [600/25883], Loss: (2.8210, 2.6909, 0.1301), Perplexity: 16.79329\n",
      "Epoch [2/3], Step [700/25883], Loss: (2.8993, 2.7380, 0.1613), Perplexity: 18.1611\n",
      "Epoch [2/3], Step [800/25883], Loss: (2.9068, 2.6243, 0.2824), Perplexity: 18.2975\n",
      "Epoch [2/3], Step [900/25883], Loss: (3.0730, 2.7329, 0.3401), Perplexity: 21.6067\n",
      "Epoch [2/3], Step [1000/25883], Loss: (2.3215, 2.2005, 0.1210), Perplexity: 10.1913\n",
      "Epoch [2/3], Step [1100/25883], Loss: (2.6363, 2.4175, 0.2188), Perplexity: 13.96122\n",
      "Epoch [2/3], Step [1200/25883], Loss: (2.7674, 2.5414, 0.2260), Perplexity: 15.9178\n",
      "Epoch [2/3], Step [1300/25883], Loss: (3.2437, 2.9426, 0.3011), Perplexity: 25.6279\n",
      "Epoch [2/3], Step [1400/25883], Loss: (4.2603, 3.6516, 0.6087), Perplexity: 70.8282\n",
      "Epoch [2/3], Step [1500/25883], Loss: (2.9525, 2.6723, 0.2803), Perplexity: 19.1544\n",
      "Epoch [2/3], Step [1600/25883], Loss: (2.7274, 2.4734, 0.2541), Perplexity: 15.2936\n",
      "Epoch [2/3], Step [1700/25883], Loss: (2.6941, 2.3778, 0.3163), Perplexity: 14.79272\n",
      "Epoch [2/3], Step [1800/25883], Loss: (2.5808, 2.4170, 0.1638), Perplexity: 13.2071\n",
      "Epoch [2/3], Step [1900/25883], Loss: (2.7476, 2.4785, 0.2691), Perplexity: 15.6054\n",
      "Epoch [2/3], Step [2000/25883], Loss: (2.7951, 2.5059, 0.2893), Perplexity: 16.3650\n",
      "Epoch [2/3], Step [2100/25883], Loss: (2.8795, 2.3627, 0.5167), Perplexity: 17.80468\n",
      "Epoch [2/3], Step [2200/25883], Loss: (2.5295, 2.3575, 0.1719), Perplexity: 12.5467\n",
      "Epoch [2/3], Step [2300/25883], Loss: (2.5172, 2.3600, 0.1572), Perplexity: 12.39424\n",
      "Epoch [2/3], Step [2400/25883], Loss: (2.3983, 2.1802, 0.2181), Perplexity: 11.0045\n",
      "Epoch [2/3], Step [2500/25883], Loss: (2.3817, 2.0142, 0.3675), Perplexity: 10.8235\n",
      "Epoch [2/3], Step [2600/25883], Loss: (3.3964, 3.0803, 0.3161), Perplexity: 29.8551\n",
      "Epoch [2/3], Step [2700/25883], Loss: (2.5906, 2.4335, 0.1571), Perplexity: 13.3379\n",
      "Epoch [2/3], Step [2800/25883], Loss: (2.9685, 2.7731, 0.1953), Perplexity: 19.4617\n",
      "Epoch [2/3], Step [2900/25883], Loss: (3.6156, 3.2728, 0.3427), Perplexity: 37.1723\n",
      "Epoch [2/3], Step [3000/25883], Loss: (3.2869, 3.0591, 0.2278), Perplexity: 26.7605\n",
      "Epoch [2/3], Step [3100/25883], Loss: (2.9438, 2.6925, 0.2513), Perplexity: 18.9877\n",
      "Epoch [2/3], Step [3200/25883], Loss: (3.2924, 2.8289, 0.4635), Perplexity: 26.9075\n",
      "Epoch [2/3], Step [3300/25883], Loss: (2.8539, 2.6596, 0.1943), Perplexity: 17.3551\n",
      "Epoch [2/3], Step [3400/25883], Loss: (3.6808, 2.5721, 1.1088), Perplexity: 39.6800\n",
      "Epoch [2/3], Step [3500/25883], Loss: (2.5978, 2.4337, 0.1641), Perplexity: 13.43452\n",
      "Epoch [2/3], Step [3600/25883], Loss: (3.1421, 2.8449, 0.2972), Perplexity: 23.1515\n",
      "Epoch [2/3], Step [3700/25883], Loss: (2.8496, 2.5267, 0.3229), Perplexity: 17.28082\n",
      "Epoch [2/3], Step [3800/25883], Loss: (3.3469, 2.7760, 0.5708), Perplexity: 28.4133\n",
      "Epoch [2/3], Step [3900/25883], Loss: (2.8845, 2.7499, 0.1346), Perplexity: 17.8939\n",
      "Epoch [2/3], Step [4000/25883], Loss: (2.8611, 2.3565, 0.5045), Perplexity: 17.4803\n",
      "Epoch [2/3], Step [4100/25883], Loss: (2.8652, 2.5512, 0.3140), Perplexity: 17.5523\n",
      "Epoch [2/3], Step [4200/25883], Loss: (2.3489, 2.2287, 0.1202), Perplexity: 10.4745\n",
      "Epoch [2/3], Step [4300/25883], Loss: (2.4159, 2.3036, 0.1123), Perplexity: 11.2002\n",
      "Epoch [2/3], Step [4400/25883], Loss: (2.8407, 2.7244, 0.1164), Perplexity: 17.1278\n",
      "Epoch [2/3], Step [4500/25883], Loss: (3.7794, 3.0925, 0.6869), Perplexity: 43.7893\n",
      "Epoch [2/3], Step [4600/25883], Loss: (3.2406, 2.7605, 0.4801), Perplexity: 25.5502\n",
      "Epoch [2/3], Step [4700/25883], Loss: (2.6557, 2.4493, 0.2065), Perplexity: 14.23565\n",
      "Epoch [2/3], Step [4800/25883], Loss: (3.6674, 3.3187, 0.3488), Perplexity: 39.1513\n",
      "Epoch [2/3], Step [4900/25883], Loss: (2.9548, 2.5598, 0.3950), Perplexity: 19.19872\n",
      "Epoch [2/3], Step [5000/25883], Loss: (2.4999, 2.3312, 0.1687), Perplexity: 12.1815\n",
      "Epoch [2/3], Step [5100/25883], Loss: (2.8767, 2.6697, 0.2070), Perplexity: 17.7550\n",
      "Epoch [2/3], Step [5200/25883], Loss: (3.2208, 2.5118, 0.7090), Perplexity: 25.0476\n",
      "Epoch [2/3], Step [5300/25883], Loss: (2.7690, 2.5844, 0.1846), Perplexity: 15.94305\n",
      "Epoch [2/3], Step [5400/25883], Loss: (2.7205, 2.6044, 0.1161), Perplexity: 15.1880\n",
      "Epoch [2/3], Step [5500/25883], Loss: (2.9541, 2.4809, 0.4732), Perplexity: 19.1849\n",
      "Epoch [2/3], Step [5600/25883], Loss: (3.2471, 2.8264, 0.4206), Perplexity: 25.7147\n",
      "Epoch [2/3], Step [5700/25883], Loss: (2.8815, 2.6251, 0.2564), Perplexity: 17.8401\n",
      "Epoch [2/3], Step [5800/25883], Loss: (2.9176, 2.6011, 0.3165), Perplexity: 18.4964\n",
      "Epoch [2/3], Step [5900/25883], Loss: (2.8104, 2.5407, 0.2698), Perplexity: 16.6173\n",
      "Epoch [2/3], Step [6000/25883], Loss: (2.5849, 2.3299, 0.2551), Perplexity: 13.2624\n",
      "Epoch [2/3], Step [6100/25883], Loss: (2.8341, 2.4785, 0.3556), Perplexity: 17.0146\n",
      "Epoch [2/3], Step [6200/25883], Loss: (2.5037, 2.2569, 0.2468), Perplexity: 12.2279\n",
      "Epoch [2/3], Step [6300/25883], Loss: (2.2951, 2.1059, 0.1891), Perplexity: 9.92507\n",
      "Epoch [2/3], Step [6400/25883], Loss: (2.5711, 2.1825, 0.3886), Perplexity: 13.08000\n",
      "Epoch [2/3], Step [6500/25883], Loss: (2.8509, 2.6687, 0.1822), Perplexity: 17.30363\n",
      "Epoch [2/3], Step [6600/25883], Loss: (3.0517, 2.5548, 0.4969), Perplexity: 21.1511\n",
      "Epoch [2/3], Step [6700/25883], Loss: (2.9366, 2.5568, 0.3798), Perplexity: 18.8514\n",
      "Epoch [2/3], Step [6800/25883], Loss: (2.6583, 2.4855, 0.1728), Perplexity: 14.2721\n",
      "Epoch [2/3], Step [6900/25883], Loss: (3.7008, 3.4369, 0.2639), Perplexity: 40.4801\n",
      "Epoch [2/3], Step [7000/25883], Loss: (3.2307, 2.7791, 0.4516), Perplexity: 25.2967\n",
      "Epoch [2/3], Step [7100/25883], Loss: (3.1087, 2.5541, 0.5547), Perplexity: 22.3925\n",
      "Epoch [2/3], Step [7200/25883], Loss: (2.7550, 2.4854, 0.2696), Perplexity: 15.7204\n",
      "Epoch [2/3], Step [7300/25883], Loss: (2.5703, 2.4277, 0.1426), Perplexity: 13.0695\n",
      "Epoch [2/3], Step [7400/25883], Loss: (2.8714, 2.6399, 0.2315), Perplexity: 17.6609\n",
      "Epoch [2/3], Step [7500/25883], Loss: (3.7998, 3.3789, 0.4209), Perplexity: 44.6925\n",
      "Epoch [2/3], Step [7600/25883], Loss: (2.9287, 2.8251, 0.1036), Perplexity: 18.7033\n",
      "Epoch [2/3], Step [7700/25883], Loss: (2.7327, 2.4898, 0.2430), Perplexity: 15.3751\n",
      "Epoch [2/3], Step [7800/25883], Loss: (2.8180, 2.4999, 0.3181), Perplexity: 16.7439\n",
      "Epoch [2/3], Step [7900/25883], Loss: (2.5582, 2.4046, 0.1536), Perplexity: 12.9129\n",
      "Epoch [2/3], Step [8000/25883], Loss: (3.1343, 2.9796, 0.1547), Perplexity: 22.9735\n",
      "Epoch [2/3], Step [8100/25883], Loss: (2.3028, 2.1180, 0.1848), Perplexity: 10.0018\n",
      "Epoch [2/3], Step [8200/25883], Loss: (2.8509, 2.5393, 0.3116), Perplexity: 17.3027\n",
      "Epoch [2/3], Step [8300/25883], Loss: (2.7304, 2.5287, 0.2017), Perplexity: 15.3386\n",
      "Epoch [2/3], Step [8400/25883], Loss: (2.3774, 2.2152, 0.1622), Perplexity: 10.7763\n",
      "Epoch [2/3], Step [8500/25883], Loss: (2.8154, 2.4673, 0.3481), Perplexity: 16.7001\n",
      "Epoch [2/3], Step [8600/25883], Loss: (2.9550, 2.7742, 0.1808), Perplexity: 19.20152\n",
      "Epoch [2/3], Step [8700/25883], Loss: (2.4382, 2.2519, 0.1862), Perplexity: 11.4519\n",
      "Epoch [2/3], Step [8800/25883], Loss: (2.7689, 2.2974, 0.4715), Perplexity: 15.9418\n",
      "Epoch [2/3], Step [8900/25883], Loss: (3.3978, 3.2609, 0.1369), Perplexity: 29.8985\n",
      "Epoch [2/3], Step [9000/25883], Loss: (2.8196, 2.6167, 0.2029), Perplexity: 16.76983\n",
      "Epoch [2/3], Step [9100/25883], Loss: (2.9174, 2.6649, 0.2525), Perplexity: 18.4939\n",
      "Epoch [2/3], Step [9200/25883], Loss: (2.5295, 2.1739, 0.3555), Perplexity: 12.5466\n",
      "Epoch [2/3], Step [9300/25883], Loss: (2.9546, 2.6693, 0.2854), Perplexity: 19.1944\n",
      "Epoch [2/3], Step [9400/25883], Loss: (3.1325, 2.9059, 0.2266), Perplexity: 22.9319\n",
      "Epoch [2/3], Step [9500/25883], Loss: (2.4488, 2.2279, 0.2209), Perplexity: 11.5745\n",
      "Epoch [2/3], Step [9600/25883], Loss: (2.7855, 2.5394, 0.2461), Perplexity: 16.2081\n",
      "Epoch [2/3], Step [9700/25883], Loss: (2.7257, 2.4162, 0.3095), Perplexity: 15.2669\n",
      "Epoch [2/3], Step [9800/25883], Loss: (2.4956, 2.2206, 0.2751), Perplexity: 12.1291\n",
      "Epoch [2/3], Step [9900/25883], Loss: (3.4991, 2.9245, 0.5746), Perplexity: 33.0862\n",
      "Epoch [2/3], Step [10000/25883], Loss: (2.4781, 2.3092, 0.1689), Perplexity: 11.9186\n",
      "Epoch [2/3], Step [10100/25883], Loss: (2.4928, 2.2950, 0.1978), Perplexity: 12.0947\n",
      "Epoch [2/3], Step [10200/25883], Loss: (2.9966, 2.6953, 0.3013), Perplexity: 20.0167\n",
      "Epoch [2/3], Step [10300/25883], Loss: (2.2300, 2.1345, 0.0955), Perplexity: 9.300391\n",
      "Epoch [2/3], Step [10400/25883], Loss: (2.6346, 2.3527, 0.2819), Perplexity: 13.9377\n",
      "Epoch [2/3], Step [10500/25883], Loss: (2.5614, 2.4482, 0.1132), Perplexity: 12.9538\n",
      "Epoch [2/3], Step [10600/25883], Loss: (2.6155, 2.4861, 0.1294), Perplexity: 13.6743\n",
      "Epoch [2/3], Step [10700/25883], Loss: (2.5576, 2.3732, 0.1844), Perplexity: 12.9053\n",
      "Epoch [2/3], Step [10800/25883], Loss: (3.2084, 2.9924, 0.2160), Perplexity: 24.7396\n",
      "Epoch [2/3], Step [10900/25883], Loss: (2.7316, 2.4682, 0.2634), Perplexity: 15.3568\n",
      "Epoch [2/3], Step [11000/25883], Loss: (3.3190, 3.0269, 0.2921), Perplexity: 27.6341\n",
      "Epoch [2/3], Step [11100/25883], Loss: (2.8709, 2.6362, 0.2347), Perplexity: 17.6534\n",
      "Epoch [2/3], Step [11200/25883], Loss: (2.0301, 1.8221, 0.2079), Perplexity: 7.61451\n",
      "Epoch [2/3], Step [11300/25883], Loss: (2.5574, 2.1748, 0.3826), Perplexity: 12.9018\n",
      "Epoch [2/3], Step [11400/25883], Loss: (2.8841, 2.7091, 0.1749), Perplexity: 17.8868\n",
      "Epoch [2/3], Step [11500/25883], Loss: (3.2639, 2.8763, 0.3877), Perplexity: 26.1515\n",
      "Epoch [2/3], Step [11600/25883], Loss: (2.7327, 2.4011, 0.3316), Perplexity: 15.3739\n",
      "Epoch [2/3], Step [11700/25883], Loss: (3.0240, 2.5596, 0.4643), Perplexity: 20.5727\n",
      "Epoch [2/3], Step [11800/25883], Loss: (2.1751, 2.1469, 0.0282), Perplexity: 8.80326\n",
      "Epoch [2/3], Step [11900/25883], Loss: (2.3217, 1.9608, 0.3608), Perplexity: 10.1926\n",
      "Epoch [2/3], Step [12000/25883], Loss: (2.9797, 2.3618, 0.6180), Perplexity: 19.6829\n",
      "Epoch [2/3], Step [12100/25883], Loss: (2.7393, 2.5010, 0.2382), Perplexity: 15.4756\n",
      "Epoch [2/3], Step [12200/25883], Loss: (3.0426, 2.7617, 0.2808), Perplexity: 20.9592\n",
      "Epoch [2/3], Step [12300/25883], Loss: (2.5217, 2.4335, 0.0881), Perplexity: 12.4493\n",
      "Epoch [2/3], Step [12400/25883], Loss: (2.4822, 2.1514, 0.3308), Perplexity: 11.9670\n",
      "Epoch [2/3], Step [12500/25883], Loss: (2.5847, 2.3491, 0.2356), Perplexity: 13.2599\n",
      "Epoch [2/3], Step [12600/25883], Loss: (2.7884, 2.5993, 0.1891), Perplexity: 16.2549\n",
      "Epoch [2/3], Step [12700/25883], Loss: (3.6069, 3.4846, 0.1223), Perplexity: 36.8514\n",
      "Epoch [2/3], Step [12800/25883], Loss: (2.4080, 2.0827, 0.3253), Perplexity: 11.1122\n",
      "Epoch [2/3], Step [12900/25883], Loss: (2.4743, 2.3264, 0.1479), Perplexity: 11.8740\n",
      "Epoch [2/3], Step [13000/25883], Loss: (2.8750, 2.6235, 0.2515), Perplexity: 17.7256\n",
      "Epoch [2/3], Step [13100/25883], Loss: (2.7526, 2.5346, 0.2180), Perplexity: 15.6830\n",
      "Epoch [2/3], Step [13200/25883], Loss: (2.3664, 2.2360, 0.1304), Perplexity: 10.6593\n",
      "Epoch [2/3], Step [13300/25883], Loss: (2.7902, 2.6386, 0.1516), Perplexity: 16.2839\n",
      "Epoch [2/3], Step [13400/25883], Loss: (2.6078, 2.4412, 0.1666), Perplexity: 13.56926\n",
      "Epoch [2/3], Step [13500/25883], Loss: (2.8423, 2.6857, 0.1566), Perplexity: 17.1553\n",
      "Epoch [2/3], Step [13600/25883], Loss: (3.1854, 2.8154, 0.3700), Perplexity: 24.1772\n",
      "Epoch [2/3], Step [13700/25883], Loss: (2.4344, 2.2735, 0.1609), Perplexity: 11.4085\n",
      "Epoch [2/3], Step [13800/25883], Loss: (2.9783, 2.7944, 0.1839), Perplexity: 19.6537\n",
      "Epoch [2/3], Step [13900/25883], Loss: (2.0761, 2.0149, 0.0612), Perplexity: 7.97305\n",
      "Epoch [2/3], Step [14000/25883], Loss: (2.5323, 2.3731, 0.1592), Perplexity: 12.5823\n",
      "Epoch [2/3], Step [14100/25883], Loss: (3.4700, 2.6540, 0.8160), Perplexity: 32.13768\n",
      "Epoch [2/3], Step [14200/25883], Loss: (3.3476, 2.4639, 0.8837), Perplexity: 28.4341\n",
      "Epoch [2/3], Step [14300/25883], Loss: (2.8407, 2.6371, 0.2036), Perplexity: 17.1280\n",
      "Epoch [2/3], Step [14400/25883], Loss: (2.9572, 2.7382, 0.2191), Perplexity: 19.2449\n",
      "Epoch [2/3], Step [14500/25883], Loss: (3.1173, 2.8534, 0.2639), Perplexity: 22.5861\n",
      "Epoch [2/3], Step [14600/25883], Loss: (2.5551, 2.5281, 0.0270), Perplexity: 12.8727\n",
      "Epoch [2/3], Step [14700/25883], Loss: (3.0178, 2.7834, 0.2344), Perplexity: 20.4463\n",
      "Epoch [2/3], Step [14800/25883], Loss: (2.8808, 2.6526, 0.2282), Perplexity: 17.8281\n",
      "Epoch [2/3], Step [14900/25883], Loss: (2.8309, 2.6149, 0.2160), Perplexity: 16.9615\n",
      "Epoch [2/3], Step [15000/25883], Loss: (2.4471, 2.2743, 0.1728), Perplexity: 11.5547\n",
      "Epoch [2/3], Step [15100/25883], Loss: (2.5427, 2.3234, 0.2193), Perplexity: 12.7137\n",
      "Epoch [2/3], Step [15200/25883], Loss: (2.8763, 2.5544, 0.3219), Perplexity: 17.7492\n",
      "Epoch [2/3], Step [15300/25883], Loss: (2.9446, 2.4718, 0.4729), Perplexity: 19.0040\n",
      "Epoch [2/3], Step [15400/25883], Loss: (2.7939, 2.6132, 0.1806), Perplexity: 16.3439\n",
      "Epoch [2/3], Step [15500/25883], Loss: (3.4879, 2.8176, 0.6702), Perplexity: 32.71568\n",
      "Epoch [2/3], Step [15600/25883], Loss: (2.5600, 2.3909, 0.1691), Perplexity: 12.9356\n",
      "Epoch [2/3], Step [15700/25883], Loss: (2.9218, 2.6191, 0.3027), Perplexity: 18.5738\n",
      "Epoch [2/3], Step [15800/25883], Loss: (2.8107, 2.7069, 0.1038), Perplexity: 16.6221\n",
      "Epoch [2/3], Step [15900/25883], Loss: (2.9012, 2.2273, 0.6739), Perplexity: 18.1958\n",
      "Epoch [2/3], Step [16000/25883], Loss: (3.0408, 2.8227, 0.2181), Perplexity: 20.9224\n",
      "Epoch [2/3], Step [16100/25883], Loss: (2.7687, 2.4727, 0.2960), Perplexity: 15.9377\n",
      "Epoch [2/3], Step [16200/25883], Loss: (3.7719, 2.7712, 1.0008), Perplexity: 43.4640\n",
      "Epoch [2/3], Step [16300/25883], Loss: (2.7818, 2.4875, 0.2943), Perplexity: 16.1473\n",
      "Epoch [2/3], Step [16400/25883], Loss: (2.6059, 2.4045, 0.2013), Perplexity: 13.5432\n",
      "Epoch [2/3], Step [16500/25883], Loss: (2.7657, 2.6397, 0.1260), Perplexity: 15.8896\n",
      "Epoch [2/3], Step [16600/25883], Loss: (3.0282, 2.8281, 0.2001), Perplexity: 20.6598\n",
      "Epoch [2/3], Step [16700/25883], Loss: (2.2112, 1.9773, 0.2340), Perplexity: 9.12709\n",
      "Epoch [2/3], Step [16800/25883], Loss: (2.8885, 2.6547, 0.2338), Perplexity: 17.9664\n",
      "Epoch [2/3], Step [16900/25883], Loss: (2.8016, 2.6884, 0.1131), Perplexity: 16.4702\n",
      "Epoch [2/3], Step [17000/25883], Loss: (3.2987, 3.0852, 0.2135), Perplexity: 27.0774\n",
      "Epoch [2/3], Step [17100/25883], Loss: (2.4056, 2.2205, 0.1850), Perplexity: 11.0848\n",
      "Epoch [2/3], Step [17200/25883], Loss: (2.5934, 2.4805, 0.1129), Perplexity: 13.3755\n",
      "Epoch [2/3], Step [17300/25883], Loss: (2.2645, 2.2222, 0.0422), Perplexity: 9.62580\n",
      "Epoch [2/3], Step [17400/25883], Loss: (2.3338, 2.0083, 0.3255), Perplexity: 10.3169\n",
      "Epoch [2/3], Step [17500/25883], Loss: (2.9429, 2.6902, 0.2527), Perplexity: 18.9709\n",
      "Epoch [2/3], Step [17600/25883], Loss: (2.5873, 2.3021, 0.2853), Perplexity: 13.2944\n",
      "Epoch [2/3], Step [17700/25883], Loss: (3.3263, 3.1429, 0.1834), Perplexity: 27.8342\n",
      "Epoch [2/3], Step [17800/25883], Loss: (2.3137, 2.0966, 0.2171), Perplexity: 10.1120\n",
      "Epoch [2/3], Step [17900/25883], Loss: (2.5046, 2.4553, 0.0493), Perplexity: 12.2381\n",
      "Epoch [2/3], Step [18000/25883], Loss: (2.5623, 2.3152, 0.2471), Perplexity: 12.9659\n",
      "Epoch [2/3], Step [18100/25883], Loss: (2.9679, 2.2471, 0.7208), Perplexity: 19.4505\n",
      "Epoch [2/3], Step [18200/25883], Loss: (3.5691, 2.6832, 0.8859), Perplexity: 35.4839\n",
      "Epoch [2/3], Step [18300/25883], Loss: (2.6306, 2.3920, 0.2386), Perplexity: 13.8822\n",
      "Epoch [2/3], Step [18400/25883], Loss: (2.3990, 2.3410, 0.0580), Perplexity: 11.0121\n",
      "Epoch [2/3], Step [18500/25883], Loss: (2.4415, 2.2217, 0.2198), Perplexity: 11.4905\n",
      "Epoch [2/3], Step [18600/25883], Loss: (2.6681, 2.5238, 0.1443), Perplexity: 14.4124\n",
      "Epoch [2/3], Step [18700/25883], Loss: (2.6724, 2.3657, 0.3067), Perplexity: 14.4744\n",
      "Epoch [2/3], Step [18800/25883], Loss: (2.6049, 2.3742, 0.2307), Perplexity: 13.5294\n",
      "Epoch [2/3], Step [18900/25883], Loss: (2.8437, 2.5915, 0.2521), Perplexity: 17.1788\n",
      "Epoch [2/3], Step [19000/25883], Loss: (2.9537, 2.8121, 0.1416), Perplexity: 19.1769\n",
      "Epoch [2/3], Step [19100/25883], Loss: (2.9198, 2.6411, 0.2787), Perplexity: 18.5373\n",
      "Epoch [2/3], Step [19200/25883], Loss: (2.5465, 2.3509, 0.1957), Perplexity: 12.7628\n",
      "Epoch [2/3], Step [19300/25883], Loss: (2.2124, 2.0584, 0.1540), Perplexity: 9.13754\n",
      "Epoch [2/3], Step [19400/25883], Loss: (2.9027, 2.7284, 0.1744), Perplexity: 18.2240\n",
      "Epoch [2/3], Step [19500/25883], Loss: (2.8309, 2.6650, 0.1658), Perplexity: 16.9601\n",
      "Epoch [2/3], Step [19600/25883], Loss: (2.7457, 2.3420, 0.4037), Perplexity: 15.5761\n",
      "Epoch [2/3], Step [19700/25883], Loss: (2.6148, 2.3606, 0.2542), Perplexity: 13.6651\n",
      "Epoch [2/3], Step [19800/25883], Loss: (2.5270, 2.2885, 0.2385), Perplexity: 12.51589\n",
      "Epoch [2/3], Step [19900/25883], Loss: (2.2181, 1.9155, 0.3026), Perplexity: 9.18970\n",
      "Epoch [2/3], Step [20000/25883], Loss: (2.9041, 2.6831, 0.2210), Perplexity: 18.2492\n",
      "Epoch [2/3], Step [20100/25883], Loss: (2.2203, 2.0995, 0.1208), Perplexity: 9.21029\n",
      "Epoch [2/3], Step [20200/25883], Loss: (2.7124, 2.3472, 0.3651), Perplexity: 15.0646\n",
      "Epoch [2/3], Step [20300/25883], Loss: (3.3756, 2.5255, 0.8500), Perplexity: 29.2408\n",
      "Epoch [2/3], Step [20400/25883], Loss: (2.2347, 2.0269, 0.2078), Perplexity: 9.34347\n",
      "Epoch [2/3], Step [20500/25883], Loss: (2.9092, 2.7361, 0.1731), Perplexity: 18.3429\n",
      "Epoch [2/3], Step [20600/25883], Loss: (2.1912, 2.0259, 0.1653), Perplexity: 8.94595\n",
      "Epoch [2/3], Step [20700/25883], Loss: (2.8792, 2.6649, 0.2143), Perplexity: 17.7997\n",
      "Epoch [2/3], Step [20800/25883], Loss: (2.8749, 2.3597, 0.5152), Perplexity: 17.7231\n",
      "Epoch [2/3], Step [20900/25883], Loss: (2.3863, 2.1432, 0.2432), Perplexity: 10.8736\n",
      "Epoch [2/3], Step [21000/25883], Loss: (2.9451, 2.7531, 0.1920), Perplexity: 19.0134\n",
      "Epoch [2/3], Step [21100/25883], Loss: (3.3524, 3.1644, 0.1880), Perplexity: 28.5701\n",
      "Epoch [2/3], Step [21200/25883], Loss: (3.6289, 3.4077, 0.2212), Perplexity: 37.6723\n",
      "Epoch [2/3], Step [21300/25883], Loss: (3.6147, 2.5345, 1.0802), Perplexity: 37.1412\n",
      "Epoch [2/3], Step [21400/25883], Loss: (2.3092, 2.2014, 0.1078), Perplexity: 10.0660\n",
      "Epoch [2/3], Step [21500/25883], Loss: (2.7607, 2.5550, 0.2056), Perplexity: 15.8106\n",
      "Epoch [2/3], Step [21600/25883], Loss: (2.5387, 2.2822, 0.2565), Perplexity: 12.6638\n",
      "Epoch [2/3], Step [21700/25883], Loss: (2.8493, 2.6150, 0.2343), Perplexity: 17.2765\n",
      "Epoch [2/3], Step [21800/25883], Loss: (2.4867, 2.2745, 0.2122), Perplexity: 12.02100\n",
      "Epoch [2/3], Step [21900/25883], Loss: (2.6252, 2.4737, 0.1515), Perplexity: 13.8067\n",
      "Epoch [2/3], Step [22000/25883], Loss: (2.5951, 2.3586, 0.2365), Perplexity: 13.3982\n",
      "Epoch [2/3], Step [22100/25883], Loss: (2.3813, 2.1855, 0.1958), Perplexity: 10.8191\n",
      "Epoch [2/3], Step [22200/25883], Loss: (2.6866, 2.3992, 0.2873), Perplexity: 14.68132\n",
      "Epoch [2/3], Step [22300/25883], Loss: (2.6311, 2.3446, 0.2865), Perplexity: 13.8895\n",
      "Epoch [2/3], Step [22400/25883], Loss: (2.9179, 2.7083, 0.2096), Perplexity: 18.5026\n",
      "Epoch [2/3], Step [22500/25883], Loss: (2.4013, 2.1630, 0.2383), Perplexity: 11.0372\n",
      "Epoch [2/3], Step [22600/25883], Loss: (2.8571, 2.6911, 0.1660), Perplexity: 17.41149\n",
      "Epoch [2/3], Step [22700/25883], Loss: (2.6752, 2.4100, 0.2652), Perplexity: 14.5158\n",
      "Epoch [2/3], Step [22800/25883], Loss: (2.6087, 2.2836, 0.3251), Perplexity: 13.5813\n",
      "Epoch [2/3], Step [22900/25883], Loss: (2.6018, 2.4226, 0.1792), Perplexity: 13.4883\n",
      "Epoch [2/3], Step [23000/25883], Loss: (3.1018, 2.8908, 0.2110), Perplexity: 22.2370\n",
      "Epoch [2/3], Step [23100/25883], Loss: (2.7437, 2.5918, 0.1519), Perplexity: 15.5448\n",
      "Epoch [2/3], Step [23200/25883], Loss: (2.8455, 2.5412, 0.3043), Perplexity: 17.2102\n",
      "Epoch [2/3], Step [23300/25883], Loss: (2.7224, 2.4328, 0.2895), Perplexity: 15.2161\n",
      "Epoch [2/3], Step [23400/25883], Loss: (2.9620, 2.8920, 0.0700), Perplexity: 19.3358\n",
      "Epoch [2/3], Step [23500/25883], Loss: (3.1118, 2.7579, 0.3539), Perplexity: 22.4604\n",
      "Epoch [2/3], Step [23600/25883], Loss: (2.7033, 2.4729, 0.2304), Perplexity: 14.9290\n",
      "Epoch [2/3], Step [23700/25883], Loss: (3.4137, 2.4279, 0.9858), Perplexity: 30.3785\n",
      "Epoch [2/3], Step [23800/25883], Loss: (2.3650, 2.1842, 0.1808), Perplexity: 10.6443\n",
      "Epoch [2/3], Step [23900/25883], Loss: (2.3862, 2.2650, 0.1213), Perplexity: 10.8725\n",
      "Epoch [2/3], Step [24000/25883], Loss: (2.7190, 2.5416, 0.1774), Perplexity: 15.1648\n",
      "Epoch [2/3], Step [24100/25883], Loss: (2.7163, 2.5920, 0.1243), Perplexity: 15.1247\n",
      "Epoch [2/3], Step [24200/25883], Loss: (2.0010, 1.8593, 0.1417), Perplexity: 7.39630\n",
      "Epoch [2/3], Step [24300/25883], Loss: (2.9092, 2.5085, 0.4007), Perplexity: 18.3423\n",
      "Epoch [2/3], Step [24400/25883], Loss: (2.3884, 2.1379, 0.2504), Perplexity: 10.8959\n",
      "Epoch [2/3], Step [24500/25883], Loss: (2.3257, 2.1872, 0.1386), Perplexity: 10.2342\n",
      "Epoch [2/3], Step [24600/25883], Loss: (2.7217, 2.5076, 0.2142), Perplexity: 15.2068\n",
      "Epoch [2/3], Step [24700/25883], Loss: (2.2350, 1.9659, 0.2692), Perplexity: 9.34693\n",
      "Epoch [2/3], Step [24800/25883], Loss: (2.4268, 2.1188, 0.3080), Perplexity: 11.3232\n",
      "Epoch [2/3], Step [24900/25883], Loss: (2.8947, 2.2283, 0.6663), Perplexity: 18.0775\n",
      "Epoch [2/3], Step [25000/25883], Loss: (2.7792, 2.5188, 0.2604), Perplexity: 16.1069\n",
      "Epoch [2/3], Step [25100/25883], Loss: (2.9657, 2.6544, 0.3114), Perplexity: 19.4090\n",
      "Epoch [2/3], Step [25200/25883], Loss: (2.8006, 2.6666, 0.1340), Perplexity: 16.4544\n",
      "Epoch [2/3], Step [25300/25883], Loss: (3.1417, 2.8924, 0.2493), Perplexity: 23.1430\n",
      "Epoch [2/3], Step [25400/25883], Loss: (2.8728, 2.7068, 0.1660), Perplexity: 17.6867\n",
      "Epoch [2/3], Step [25500/25883], Loss: (2.1155, 2.0499, 0.0657), Perplexity: 8.29393\n",
      "Epoch [2/3], Step [25600/25883], Loss: (2.6948, 2.2269, 0.4679), Perplexity: 14.8024\n",
      "Epoch [2/3], Step [25700/25883], Loss: (2.6480, 2.2010, 0.4470), Perplexity: 14.1255\n",
      "Epoch [2/3], Step [25800/25883], Loss: (2.9478, 2.6679, 0.2798), Perplexity: 19.0630\n",
      "Epoch [3/3], Step [100/25883], Loss: (2.3971, 1.9814, 0.4158), Perplexity: 10.991715\n",
      "Epoch [3/3], Step [200/25883], Loss: (2.4640, 2.2419, 0.2221), Perplexity: 11.7521\n",
      "Epoch [3/3], Step [300/25883], Loss: (2.2425, 2.0666, 0.1759), Perplexity: 9.41693\n",
      "Epoch [3/3], Step [400/25883], Loss: (2.9219, 2.6779, 0.2441), Perplexity: 18.5766\n",
      "Epoch [3/3], Step [500/25883], Loss: (3.0040, 2.7973, 0.2067), Perplexity: 20.1657\n",
      "Epoch [3/3], Step [600/25883], Loss: (2.3755, 2.1089, 0.2666), Perplexity: 10.7562\n",
      "Epoch [3/3], Step [700/25883], Loss: (3.1126, 2.7734, 0.3392), Perplexity: 22.4793\n",
      "Epoch [3/3], Step [800/25883], Loss: (2.3182, 2.0780, 0.2401), Perplexity: 10.1569\n",
      "Epoch [3/3], Step [900/25883], Loss: (2.5486, 2.2256, 0.3230), Perplexity: 12.7895\n",
      "Epoch [3/3], Step [1000/25883], Loss: (3.1289, 2.7629, 0.3660), Perplexity: 22.8497\n",
      "Epoch [3/3], Step [1100/25883], Loss: (2.2940, 2.1975, 0.0964), Perplexity: 9.91420\n",
      "Epoch [3/3], Step [1200/25883], Loss: (2.6462, 2.4756, 0.1706), Perplexity: 14.10042\n",
      "Epoch [3/3], Step [1300/25883], Loss: (2.4562, 2.2977, 0.1584), Perplexity: 11.6602\n",
      "Epoch [3/3], Step [1400/25883], Loss: (2.7124, 2.5856, 0.1268), Perplexity: 15.0652\n",
      "Epoch [3/3], Step [1500/25883], Loss: (2.4339, 2.2371, 0.1967), Perplexity: 11.4027\n",
      "Epoch [3/3], Step [1600/25883], Loss: (2.9307, 2.7782, 0.1526), Perplexity: 18.7416\n",
      "Epoch [3/3], Step [1700/25883], Loss: (2.1717, 1.9643, 0.2074), Perplexity: 8.77364\n",
      "Epoch [3/3], Step [1800/25883], Loss: (2.0481, 1.7742, 0.2740), Perplexity: 7.75359\n",
      "Epoch [3/3], Step [1900/25883], Loss: (2.9395, 2.7441, 0.1954), Perplexity: 18.9065\n",
      "Epoch [3/3], Step [2000/25883], Loss: (2.8593, 2.6305, 0.2288), Perplexity: 17.4501\n",
      "Epoch [3/3], Step [2100/25883], Loss: (2.3456, 2.0976, 0.2481), Perplexity: 10.4399\n",
      "Epoch [3/3], Step [2200/25883], Loss: (3.5182, 3.2739, 0.2443), Perplexity: 33.7229\n",
      "Epoch [3/3], Step [2300/25883], Loss: (2.9186, 2.4845, 0.4340), Perplexity: 18.5146\n",
      "Epoch [3/3], Step [2400/25883], Loss: (2.1003, 1.8186, 0.2817), Perplexity: 8.16867\n",
      "Epoch [3/3], Step [2500/25883], Loss: (2.6926, 2.3462, 0.3464), Perplexity: 14.7699\n",
      "Epoch [3/3], Step [2600/25883], Loss: (2.6014, 2.3711, 0.2304), Perplexity: 13.4832\n",
      "Epoch [3/3], Step [2700/25883], Loss: (2.7356, 2.4990, 0.2366), Perplexity: 15.4188\n",
      "Epoch [3/3], Step [2800/25883], Loss: (2.4301, 2.1659, 0.2642), Perplexity: 11.3599\n",
      "Epoch [3/3], Step [2900/25883], Loss: (2.5715, 2.3590, 0.2125), Perplexity: 13.0852\n",
      "Epoch [3/3], Step [3000/25883], Loss: (2.2402, 2.0055, 0.2347), Perplexity: 9.39511\n",
      "Epoch [3/3], Step [3100/25883], Loss: (2.3417, 2.1174, 0.2242), Perplexity: 10.3984\n",
      "Epoch [3/3], Step [3200/25883], Loss: (2.3887, 2.1095, 0.2792), Perplexity: 10.8997\n",
      "Epoch [3/3], Step [3300/25883], Loss: (2.9946, 2.6660, 0.3286), Perplexity: 19.9774\n",
      "Epoch [3/3], Step [3400/25883], Loss: (2.2513, 2.1838, 0.0675), Perplexity: 9.49992\n",
      "Epoch [3/3], Step [3500/25883], Loss: (3.0345, 2.3698, 0.6647), Perplexity: 20.7909\n",
      "Epoch [3/3], Step [3600/25883], Loss: (2.4536, 2.2312, 0.2224), Perplexity: 11.6305\n",
      "Epoch [3/3], Step [3700/25883], Loss: (2.5851, 2.3426, 0.2424), Perplexity: 13.2644\n",
      "Epoch [3/3], Step [3800/25883], Loss: (2.3537, 2.2024, 0.1513), Perplexity: 10.5241\n",
      "Epoch [3/3], Step [3900/25883], Loss: (2.8284, 2.5971, 0.2314), Perplexity: 16.9190\n",
      "Epoch [3/3], Step [4000/25883], Loss: (3.0461, 2.7508, 0.2952), Perplexity: 21.0329\n",
      "Epoch [3/3], Step [4100/25883], Loss: (2.1471, 1.8834, 0.2637), Perplexity: 8.56049\n",
      "Epoch [3/3], Step [4200/25883], Loss: (2.1506, 2.0015, 0.1491), Perplexity: 8.589649\n",
      "Epoch [3/3], Step [4300/25883], Loss: (3.1871, 3.0473, 0.1398), Perplexity: 24.2176\n",
      "Epoch [3/3], Step [4400/25883], Loss: (2.2748, 2.1158, 0.1590), Perplexity: 9.72581\n",
      "Epoch [3/3], Step [4500/25883], Loss: (3.0577, 2.5761, 0.4816), Perplexity: 21.2790\n",
      "Epoch [3/3], Step [4600/25883], Loss: (2.5931, 2.4925, 0.1006), Perplexity: 13.3718\n",
      "Epoch [3/3], Step [4700/25883], Loss: (2.0958, 1.9811, 0.1147), Perplexity: 8.13229\n",
      "Epoch [3/3], Step [4800/25883], Loss: (2.7864, 2.4500, 0.3365), Perplexity: 16.2229\n",
      "Epoch [3/3], Step [4900/25883], Loss: (2.5766, 2.4056, 0.1710), Perplexity: 13.1521\n",
      "Epoch [3/3], Step [5000/25883], Loss: (2.5201, 1.9887, 0.5315), Perplexity: 12.4304\n",
      "Epoch [3/3], Step [5100/25883], Loss: (2.3786, 2.0922, 0.2864), Perplexity: 10.7893\n",
      "Epoch [3/3], Step [5200/25883], Loss: (3.1850, 2.5324, 0.6526), Perplexity: 24.1670\n",
      "Epoch [3/3], Step [5300/25883], Loss: (2.2807, 2.1045, 0.1762), Perplexity: 9.78362\n",
      "Epoch [3/3], Step [5400/25883], Loss: (2.2764, 2.0546, 0.2218), Perplexity: 9.74200\n",
      "Epoch [3/3], Step [5500/25883], Loss: (2.5930, 2.4004, 0.1926), Perplexity: 13.3700\n",
      "Epoch [3/3], Step [5600/25883], Loss: (2.4998, 2.1469, 0.3530), Perplexity: 12.1803\n",
      "Epoch [3/3], Step [5700/25883], Loss: (2.7473, 2.2675, 0.4798), Perplexity: 15.6000\n",
      "Epoch [3/3], Step [5800/25883], Loss: (2.9265, 2.5924, 0.3341), Perplexity: 18.6626\n",
      "Epoch [3/3], Step [5900/25883], Loss: (3.0726, 2.4071, 0.6656), Perplexity: 21.5989\n",
      "Epoch [3/3], Step [6000/25883], Loss: (2.5946, 2.3915, 0.2032), Perplexity: 13.3917\n",
      "Epoch [3/3], Step [6100/25883], Loss: (2.3322, 2.2190, 0.1132), Perplexity: 10.3002\n",
      "Epoch [3/3], Step [6200/25883], Loss: (2.2080, 2.0596, 0.1484), Perplexity: 9.09732\n",
      "Epoch [3/3], Step [6300/25883], Loss: (2.6249, 2.3617, 0.2632), Perplexity: 13.8034\n",
      "Epoch [3/3], Step [6400/25883], Loss: (2.6315, 2.5325, 0.0990), Perplexity: 13.8949\n",
      "Epoch [3/3], Step [6500/25883], Loss: (2.8022, 2.5078, 0.2944), Perplexity: 16.4803\n",
      "Epoch [3/3], Step [6600/25883], Loss: (2.7319, 2.4893, 0.2426), Perplexity: 15.3627\n",
      "Epoch [3/3], Step [6700/25883], Loss: (2.1210, 2.0011, 0.1199), Perplexity: 8.33920\n",
      "Epoch [3/3], Step [6800/25883], Loss: (2.7791, 2.4264, 0.3526), Perplexity: 16.1043\n",
      "Epoch [3/3], Step [6900/25883], Loss: (2.6952, 2.5606, 0.1346), Perplexity: 14.8086\n",
      "Epoch [3/3], Step [7000/25883], Loss: (2.3523, 2.1174, 0.2349), Perplexity: 10.5095\n",
      "Epoch [3/3], Step [7100/25883], Loss: (2.2249, 2.0833, 0.1416), Perplexity: 9.25220\n",
      "Epoch [3/3], Step [7200/25883], Loss: (2.8978, 2.6607, 0.2372), Perplexity: 18.1345\n",
      "Epoch [3/3], Step [7300/25883], Loss: (2.4810, 2.3250, 0.1560), Perplexity: 11.9534\n",
      "Epoch [3/3], Step [7400/25883], Loss: (2.2321, 2.1035, 0.1287), Perplexity: 9.31964\n",
      "Epoch [3/3], Step [7500/25883], Loss: (2.5954, 2.4320, 0.1634), Perplexity: 13.4015\n",
      "Epoch [3/3], Step [7600/25883], Loss: (2.3310, 2.2085, 0.1225), Perplexity: 10.2880\n",
      "Epoch [3/3], Step [7700/25883], Loss: (2.1387, 2.0769, 0.0618), Perplexity: 8.48821\n",
      "Epoch [3/3], Step [7800/25883], Loss: (2.4391, 2.2662, 0.1730), Perplexity: 11.4633\n",
      "Epoch [3/3], Step [7900/25883], Loss: (2.6516, 2.3774, 0.2742), Perplexity: 14.1773\n",
      "Epoch [3/3], Step [8000/25883], Loss: (2.3485, 2.2350, 0.1136), Perplexity: 10.4703\n",
      "Epoch [3/3], Step [8100/25883], Loss: (2.6196, 2.3799, 0.2397), Perplexity: 13.7302\n",
      "Epoch [3/3], Step [8200/25883], Loss: (2.5652, 2.3255, 0.2397), Perplexity: 13.0032\n",
      "Epoch [3/3], Step [8300/25883], Loss: (2.3723, 1.9943, 0.3780), Perplexity: 10.7221\n",
      "Epoch [3/3], Step [8400/25883], Loss: (2.3062, 2.1809, 0.1253), Perplexity: 10.0360\n",
      "Epoch [3/3], Step [8500/25883], Loss: (3.1289, 2.8974, 0.2315), Perplexity: 22.8488\n",
      "Epoch [3/3], Step [8600/25883], Loss: (2.5799, 2.3847, 0.1952), Perplexity: 13.1953\n",
      "Epoch [3/3], Step [8700/25883], Loss: (2.3198, 2.0199, 0.2999), Perplexity: 10.1739\n",
      "Epoch [3/3], Step [8800/25883], Loss: (2.5197, 2.3649, 0.1548), Perplexity: 12.4254\n",
      "Epoch [3/3], Step [8900/25883], Loss: (2.2413, 1.9690, 0.2722), Perplexity: 9.40533\n",
      "Epoch [3/3], Step [9000/25883], Loss: (2.4597, 2.2826, 0.1771), Perplexity: 11.7010\n",
      "Epoch [3/3], Step [9100/25883], Loss: (2.6077, 2.3778, 0.2299), Perplexity: 13.5676\n",
      "Epoch [3/3], Step [9200/25883], Loss: (2.9961, 2.7631, 0.2331), Perplexity: 20.0080\n",
      "Epoch [3/3], Step [9300/25883], Loss: (3.2277, 3.0402, 0.1876), Perplexity: 25.2226\n",
      "Epoch [3/3], Step [9400/25883], Loss: (2.4237, 2.2462, 0.1775), Perplexity: 11.2878\n",
      "Epoch [3/3], Step [9500/25883], Loss: (2.3925, 2.2404, 0.1521), Perplexity: 10.9411\n",
      "Epoch [3/3], Step [9600/25883], Loss: (2.2299, 1.9880, 0.2420), Perplexity: 9.29930\n",
      "Epoch [3/3], Step [9700/25883], Loss: (2.6479, 2.2541, 0.3938), Perplexity: 14.1243\n",
      "Epoch [3/3], Step [9800/25883], Loss: (2.2271, 2.0765, 0.1506), Perplexity: 9.27297\n",
      "Epoch [3/3], Step [9900/25883], Loss: (2.3537, 2.1662, 0.1875), Perplexity: 10.5248\n",
      "Epoch [3/3], Step [10000/25883], Loss: (2.2705, 2.0508, 0.2197), Perplexity: 9.6845\n",
      "Epoch [3/3], Step [10100/25883], Loss: (3.0341, 2.7065, 0.3276), Perplexity: 20.7828\n",
      "Epoch [3/3], Step [10200/25883], Loss: (2.6403, 2.4205, 0.2199), Perplexity: 14.0177\n",
      "Epoch [3/3], Step [10300/25883], Loss: (2.8914, 2.2745, 0.6169), Perplexity: 18.0188\n",
      "Epoch [3/3], Step [10400/25883], Loss: (2.6186, 2.4253, 0.1933), Perplexity: 13.7169\n",
      "Epoch [3/3], Step [10500/25883], Loss: (2.7063, 2.3749, 0.3315), Perplexity: 14.9745\n",
      "Epoch [3/3], Step [10600/25883], Loss: (2.4907, 2.3921, 0.0986), Perplexity: 12.0697\n",
      "Epoch [3/3], Step [10700/25883], Loss: (2.3973, 2.2825, 0.1148), Perplexity: 10.9932\n",
      "Epoch [3/3], Step [10800/25883], Loss: (2.8595, 2.1377, 0.7218), Perplexity: 17.4528\n",
      "Epoch [3/3], Step [10900/25883], Loss: (2.1174, 1.9854, 0.1320), Perplexity: 8.30990\n",
      "Epoch [3/3], Step [11000/25883], Loss: (2.9384, 2.7838, 0.1547), Perplexity: 18.8861\n",
      "Epoch [3/3], Step [11100/25883], Loss: (2.4020, 2.1911, 0.2108), Perplexity: 11.0452\n",
      "Epoch [3/3], Step [11200/25883], Loss: (2.6654, 2.5039, 0.1615), Perplexity: 14.3743\n",
      "Epoch [3/3], Step [11300/25883], Loss: (2.6979, 2.3939, 0.3040), Perplexity: 14.8478\n",
      "Epoch [3/3], Step [11400/25883], Loss: (2.3477, 2.2133, 0.1344), Perplexity: 10.4615\n",
      "Epoch [3/3], Step [11500/25883], Loss: (2.3681, 2.2076, 0.1605), Perplexity: 10.6768\n",
      "Epoch [3/3], Step [11600/25883], Loss: (2.9161, 2.6378, 0.2783), Perplexity: 18.4700\n",
      "Epoch [3/3], Step [11700/25883], Loss: (2.5676, 2.4075, 0.1601), Perplexity: 13.0347\n",
      "Epoch [3/3], Step [11800/25883], Loss: (2.5191, 2.3182, 0.2010), Perplexity: 12.41761\n",
      "Epoch [3/3], Step [11900/25883], Loss: (2.6755, 2.4876, 0.1879), Perplexity: 14.5194\n",
      "Epoch [3/3], Step [12000/25883], Loss: (3.1820, 3.0230, 0.1590), Perplexity: 24.0949\n",
      "Epoch [3/3], Step [12100/25883], Loss: (2.2981, 2.1038, 0.1943), Perplexity: 9.95554\n",
      "Epoch [3/3], Step [12200/25883], Loss: (2.7503, 2.3816, 0.3687), Perplexity: 15.6476\n",
      "Epoch [3/3], Step [12300/25883], Loss: (2.2003, 1.7939, 0.4064), Perplexity: 9.02775\n",
      "Epoch [3/3], Step [12400/25883], Loss: (2.3936, 2.2643, 0.1293), Perplexity: 10.9533\n",
      "Epoch [3/3], Step [12500/25883], Loss: (2.5201, 2.1555, 0.3646), Perplexity: 12.4293\n",
      "Epoch [3/3], Step [12600/25883], Loss: (2.8951, 2.7083, 0.1868), Perplexity: 18.0852\n",
      "Epoch [3/3], Step [12700/25883], Loss: (2.3570, 1.9291, 0.4279), Perplexity: 10.5592\n",
      "Epoch [3/3], Step [12800/25883], Loss: (2.2724, 1.9313, 0.3410), Perplexity: 9.70246\n",
      "Epoch [3/3], Step [12900/25883], Loss: (2.1005, 1.9784, 0.1221), Perplexity: 8.17004\n",
      "Epoch [3/3], Step [13000/25883], Loss: (2.9447, 2.1875, 0.7573), Perplexity: 19.0059\n",
      "Epoch [3/3], Step [13100/25883], Loss: (2.1977, 2.0558, 0.1419), Perplexity: 9.00425\n",
      "Epoch [3/3], Step [13200/25883], Loss: (2.1832, 2.0258, 0.1574), Perplexity: 8.87486\n",
      "Epoch [3/3], Step [13300/25883], Loss: (2.3684, 2.1477, 0.2207), Perplexity: 10.6802\n",
      "Epoch [3/3], Step [13400/25883], Loss: (2.6337, 2.1866, 0.4472), Perplexity: 13.9257\n",
      "Epoch [3/3], Step [13500/25883], Loss: (2.3313, 2.1655, 0.1658), Perplexity: 10.2913\n",
      "Epoch [3/3], Step [13600/25883], Loss: (2.6261, 2.1766, 0.4495), Perplexity: 13.8199\n",
      "Epoch [3/3], Step [13700/25883], Loss: (2.6323, 2.4790, 0.1533), Perplexity: 13.9053\n",
      "Epoch [3/3], Step [13800/25883], Loss: (2.5107, 1.9857, 0.5250), Perplexity: 12.3138\n",
      "Epoch [3/3], Step [13900/25883], Loss: (3.0461, 2.3706, 0.6755), Perplexity: 21.0326\n",
      "Epoch [3/3], Step [14000/25883], Loss: (2.6563, 2.2038, 0.4525), Perplexity: 14.2432\n",
      "Epoch [3/3], Step [14100/25883], Loss: (2.4928, 2.2206, 0.2723), Perplexity: 12.0955\n",
      "Epoch [3/3], Step [14200/25883], Loss: (2.3454, 2.1934, 0.1520), Perplexity: 10.4370\n",
      "Epoch [3/3], Step [14300/25883], Loss: (2.2502, 2.0143, 0.2359), Perplexity: 9.48989\n",
      "Epoch [3/3], Step [14400/25883], Loss: (3.1948, 2.8219, 0.3728), Perplexity: 24.4048\n",
      "Epoch [3/3], Step [14500/25883], Loss: (2.2060, 2.0407, 0.1654), Perplexity: 9.07961\n",
      "Epoch [3/3], Step [14600/25883], Loss: (2.5407, 2.3464, 0.1942), Perplexity: 12.6881\n",
      "Epoch [3/3], Step [14700/25883], Loss: (2.7901, 2.6308, 0.1593), Perplexity: 16.2829\n",
      "Epoch [3/3], Step [14800/25883], Loss: (2.3642, 2.1300, 0.2342), Perplexity: 10.6359\n",
      "Epoch [3/3], Step [14900/25883], Loss: (3.0553, 2.1589, 0.8964), Perplexity: 21.2267\n",
      "Epoch [3/3], Step [15000/25883], Loss: (2.2989, 1.9565, 0.3424), Perplexity: 9.96331\n",
      "Epoch [3/3], Step [15100/25883], Loss: (2.6566, 2.3740, 0.2826), Perplexity: 14.2471\n",
      "Epoch [3/3], Step [15200/25883], Loss: (2.6130, 2.5313, 0.0817), Perplexity: 13.6395\n",
      "Epoch [3/3], Step [15300/25883], Loss: (2.2754, 2.0323, 0.2432), Perplexity: 9.73224\n",
      "Epoch [3/3], Step [15400/25883], Loss: (2.1184, 1.9749, 0.1435), Perplexity: 8.31798\n",
      "Epoch [3/3], Step [15500/25883], Loss: (1.9251, 1.7725, 0.1526), Perplexity: 6.85614\n",
      "Epoch [3/3], Step [15600/25883], Loss: (2.3688, 2.1384, 0.2304), Perplexity: 10.6842\n",
      "Epoch [3/3], Step [15700/25883], Loss: (2.5132, 2.1448, 0.3684), Perplexity: 12.3442\n",
      "Epoch [3/3], Step [15800/25883], Loss: (2.8791, 2.5572, 0.3219), Perplexity: 17.7990\n",
      "Epoch [3/3], Step [15900/25883], Loss: (2.3755, 2.1341, 0.2414), Perplexity: 10.7568\n",
      "Epoch [3/3], Step [16000/25883], Loss: (3.1394, 2.9130, 0.2264), Perplexity: 23.09001\n",
      "Epoch [3/3], Step [16100/25883], Loss: (2.2382, 2.1857, 0.0525), Perplexity: 9.37605\n",
      "Epoch [3/3], Step [16200/25883], Loss: (2.5338, 2.3513, 0.1825), Perplexity: 12.6016\n",
      "Epoch [3/3], Step [16300/25883], Loss: (2.8148, 2.6201, 0.1948), Perplexity: 16.6904\n",
      "Epoch [3/3], Step [16400/25883], Loss: (2.1779, 1.9087, 0.2692), Perplexity: 8.82775\n",
      "Epoch [3/3], Step [16500/25883], Loss: (2.9577, 2.7711, 0.1866), Perplexity: 19.2539\n",
      "Epoch [3/3], Step [16600/25883], Loss: (3.3939, 3.0685, 0.3254), Perplexity: 29.7831\n",
      "Epoch [3/3], Step [16700/25883], Loss: (2.5689, 2.4121, 0.1568), Perplexity: 13.0514\n",
      "Epoch [3/3], Step [16800/25883], Loss: (2.7481, 2.3862, 0.3618), Perplexity: 15.61242\n",
      "Epoch [3/3], Step [16900/25883], Loss: (2.1337, 1.9994, 0.1343), Perplexity: 8.44645\n",
      "Epoch [3/3], Step [17000/25883], Loss: (2.8211, 2.4490, 0.3721), Perplexity: 16.7960\n",
      "Epoch [3/3], Step [17100/25883], Loss: (2.7339, 2.5850, 0.1490), Perplexity: 15.3935\n",
      "Epoch [3/3], Step [17200/25883], Loss: (2.5212, 2.3222, 0.1990), Perplexity: 12.4430\n",
      "Epoch [3/3], Step [17300/25883], Loss: (2.4797, 2.0996, 0.3800), Perplexity: 11.9374\n",
      "Epoch [3/3], Step [17400/25883], Loss: (2.3272, 2.2479, 0.0793), Perplexity: 10.2488\n",
      "Epoch [3/3], Step [17500/25883], Loss: (2.4015, 2.1762, 0.2254), Perplexity: 11.0401\n",
      "Epoch [3/3], Step [17600/25883], Loss: (2.5286, 2.3402, 0.1884), Perplexity: 12.5357\n",
      "Epoch [3/3], Step [17700/25883], Loss: (2.4109, 2.2273, 0.1836), Perplexity: 11.1442\n",
      "Epoch [3/3], Step [17800/25883], Loss: (2.6872, 2.5894, 0.0978), Perplexity: 14.6902\n",
      "Epoch [3/3], Step [17900/25883], Loss: (2.0838, 1.8931, 0.1906), Perplexity: 8.03469\n",
      "Epoch [3/3], Step [18000/25883], Loss: (2.0972, 1.8196, 0.2776), Perplexity: 8.14349\n",
      "Epoch [3/3], Step [18100/25883], Loss: (2.6555, 2.5422, 0.1133), Perplexity: 14.2325\n",
      "Epoch [3/3], Step [18200/25883], Loss: (2.2022, 1.9662, 0.2360), Perplexity: 9.04465\n",
      "Epoch [3/3], Step [18300/25883], Loss: (2.4745, 2.3424, 0.1321), Perplexity: 11.8763\n",
      "Epoch [3/3], Step [18400/25883], Loss: (2.1979, 1.9723, 0.2256), Perplexity: 9.00630\n",
      "Epoch [3/3], Step [18500/25883], Loss: (2.7756, 2.6094, 0.1663), Perplexity: 16.0486\n",
      "Epoch [3/3], Step [18600/25883], Loss: (2.3156, 2.0253, 0.2903), Perplexity: 10.1307\n",
      "Epoch [3/3], Step [18700/25883], Loss: (2.3345, 2.0576, 0.2769), Perplexity: 10.3239\n",
      "Epoch [3/3], Step [18800/25883], Loss: (2.8830, 2.6949, 0.1882), Perplexity: 17.8681\n",
      "Epoch [3/3], Step [18900/25883], Loss: (2.7318, 2.3435, 0.3883), Perplexity: 15.3604\n",
      "Epoch [3/3], Step [19000/25883], Loss: (3.1258, 2.4238, 0.7020), Perplexity: 22.7778\n",
      "Epoch [3/3], Step [19100/25883], Loss: (2.3096, 2.0868, 0.2229), Perplexity: 10.07065\n",
      "Epoch [3/3], Step [19200/25883], Loss: (2.4065, 2.1787, 0.2278), Perplexity: 11.0948\n",
      "Epoch [3/3], Step [19300/25883], Loss: (2.7235, 2.5385, 0.1850), Perplexity: 15.2331\n",
      "Epoch [3/3], Step [19400/25883], Loss: (2.4224, 2.2256, 0.1967), Perplexity: 11.2724\n",
      "Epoch [3/3], Step [19500/25883], Loss: (2.8689, 2.6578, 0.2111), Perplexity: 17.6171\n",
      "Epoch [3/3], Step [19600/25883], Loss: (2.4769, 2.2883, 0.1885), Perplexity: 11.9038\n",
      "Epoch [3/3], Step [19700/25883], Loss: (2.4806, 2.3970, 0.0836), Perplexity: 11.9487\n",
      "Epoch [3/3], Step [19800/25883], Loss: (2.4880, 2.2399, 0.2481), Perplexity: 12.0374\n",
      "Epoch [3/3], Step [19900/25883], Loss: (2.2193, 1.9269, 0.2925), Perplexity: 9.20133\n",
      "Epoch [3/3], Step [20000/25883], Loss: (2.4494, 2.2853, 0.1641), Perplexity: 11.5818\n",
      "Epoch [3/3], Step [20100/25883], Loss: (2.4940, 2.1460, 0.3480), Perplexity: 12.1098\n",
      "Epoch [3/3], Step [20200/25883], Loss: (2.7103, 2.3753, 0.3349), Perplexity: 15.0335\n",
      "Epoch [3/3], Step [20300/25883], Loss: (2.5385, 2.4525, 0.0860), Perplexity: 12.6607\n",
      "Epoch [3/3], Step [20400/25883], Loss: (2.5896, 2.4383, 0.1513), Perplexity: 13.3243\n",
      "Epoch [3/3], Step [20500/25883], Loss: (2.6876, 2.5015, 0.1861), Perplexity: 14.6970\n",
      "Epoch [3/3], Step [20600/25883], Loss: (2.1963, 2.0327, 0.1636), Perplexity: 8.99137\n",
      "Epoch [3/3], Step [20700/25883], Loss: (2.4813, 2.3238, 0.1574), Perplexity: 11.9562\n",
      "Epoch [3/3], Step [20800/25883], Loss: (2.0816, 1.9191, 0.1624), Perplexity: 8.01697\n",
      "Epoch [3/3], Step [20900/25883], Loss: (2.9733, 2.5126, 0.4607), Perplexity: 19.5571\n",
      "Epoch [3/3], Step [21000/25883], Loss: (2.7752, 2.6115, 0.1637), Perplexity: 16.0420\n",
      "Epoch [3/3], Step [21100/25883], Loss: (2.6088, 2.3030, 0.3058), Perplexity: 13.5821\n",
      "Epoch [3/3], Step [21200/25883], Loss: (2.7029, 2.5282, 0.1747), Perplexity: 14.9233\n",
      "Epoch [3/3], Step [21300/25883], Loss: (2.3755, 2.1404, 0.2351), Perplexity: 10.7562\n",
      "Epoch [3/3], Step [21400/25883], Loss: (2.6901, 2.3327, 0.3573), Perplexity: 14.7326\n",
      "Epoch [3/3], Step [21500/25883], Loss: (2.3958, 2.2847, 0.1112), Perplexity: 10.9771\n",
      "Epoch [3/3], Step [21600/25883], Loss: (2.3685, 2.1191, 0.2493), Perplexity: 10.6811\n",
      "Epoch [3/3], Step [21700/25883], Loss: (2.9866, 2.1990, 0.7876), Perplexity: 19.8184\n",
      "Epoch [3/3], Step [21800/25883], Loss: (2.4927, 2.2108, 0.2819), Perplexity: 12.0940\n",
      "Epoch [3/3], Step [21900/25883], Loss: (2.5240, 1.9781, 0.5459), Perplexity: 12.4781\n",
      "Epoch [3/3], Step [22000/25883], Loss: (2.4833, 2.3053, 0.1781), Perplexity: 11.9810\n",
      "Epoch [3/3], Step [22100/25883], Loss: (2.6337, 2.5056, 0.1280), Perplexity: 13.92468\n",
      "Epoch [3/3], Step [22200/25883], Loss: (2.6546, 2.5424, 0.1122), Perplexity: 14.2193\n",
      "Epoch [3/3], Step [22300/25883], Loss: (2.1633, 1.9856, 0.1777), Perplexity: 8.69976\n",
      "Epoch [3/3], Step [22400/25883], Loss: (2.0597, 1.9853, 0.0744), Perplexity: 7.843918\n",
      "Epoch [3/3], Step [22500/25883], Loss: (2.0775, 1.8564, 0.2212), Perplexity: 7.98480\n",
      "Epoch [3/3], Step [22600/25883], Loss: (2.5536, 1.9788, 0.5749), Perplexity: 12.8537\n",
      "Epoch [3/3], Step [22700/25883], Loss: (2.3922, 2.2898, 0.1024), Perplexity: 10.9375\n",
      "Epoch [3/3], Step [22800/25883], Loss: (2.3656, 2.1849, 0.1807), Perplexity: 10.6507\n",
      "Epoch [3/3], Step [22900/25883], Loss: (2.5347, 2.3634, 0.1713), Perplexity: 12.6126\n",
      "Epoch [3/3], Step [23000/25883], Loss: (2.5209, 2.2104, 0.3104), Perplexity: 12.4396\n",
      "Epoch [3/3], Step [23100/25883], Loss: (2.8196, 2.6852, 0.1344), Perplexity: 16.7703\n",
      "Epoch [3/3], Step [23200/25883], Loss: (3.7703, 2.5615, 1.2088), Perplexity: 43.3932\n",
      "Epoch [3/3], Step [23300/25883], Loss: (2.0110, 1.9267, 0.0843), Perplexity: 7.47098\n",
      "Epoch [3/3], Step [23400/25883], Loss: (2.3755, 2.1499, 0.2256), Perplexity: 10.7559\n",
      "Epoch [3/3], Step [23500/25883], Loss: (2.4017, 2.0790, 0.3227), Perplexity: 11.0424\n",
      "Epoch [3/3], Step [23600/25883], Loss: (2.5312, 2.3968, 0.1344), Perplexity: 12.5690\n",
      "Epoch [3/3], Step [23700/25883], Loss: (2.2920, 2.1172, 0.1748), Perplexity: 9.89523\n",
      "Epoch [3/3], Step [23800/25883], Loss: (2.4948, 2.4209, 0.0738), Perplexity: 12.1190\n",
      "Epoch [3/3], Step [23900/25883], Loss: (1.9068, 1.7898, 0.1170), Perplexity: 6.73173\n",
      "Epoch [3/3], Step [24000/25883], Loss: (2.5030, 2.3528, 0.1502), Perplexity: 12.2193\n",
      "Epoch [3/3], Step [24100/25883], Loss: (2.1828, 2.0253, 0.1574), Perplexity: 8.87083\n",
      "Epoch [3/3], Step [24200/25883], Loss: (2.4736, 2.1995, 0.2741), Perplexity: 11.8650\n",
      "Epoch [3/3], Step [24300/25883], Loss: (2.0033, 1.7744, 0.2289), Perplexity: 7.41355\n",
      "Epoch [3/3], Step [24400/25883], Loss: (2.7322, 2.4912, 0.2409), Perplexity: 15.3661\n",
      "Epoch [3/3], Step [24500/25883], Loss: (2.6404, 2.4713, 0.1690), Perplexity: 14.0183\n",
      "Epoch [3/3], Step [24600/25883], Loss: (3.0028, 2.7128, 0.2899), Perplexity: 20.1411\n",
      "Epoch [3/3], Step [24700/25883], Loss: (2.6687, 2.2432, 0.4254), Perplexity: 14.4205\n",
      "Epoch [3/3], Step [24800/25883], Loss: (2.2451, 1.8320, 0.4131), Perplexity: 9.44118\n",
      "Epoch [3/3], Step [24900/25883], Loss: (3.3224, 3.0979, 0.2245), Perplexity: 27.7264\n",
      "Epoch [3/3], Step [25000/25883], Loss: (2.8690, 2.6766, 0.1924), Perplexity: 17.6191\n",
      "Epoch [3/3], Step [25100/25883], Loss: (2.7718, 2.3853, 0.3865), Perplexity: 15.9871\n",
      "Epoch [3/3], Step [25200/25883], Loss: (2.2028, 2.0902, 0.1126), Perplexity: 9.04996\n",
      "Epoch [3/3], Step [25300/25883], Loss: (2.9398, 2.8808, 0.0590), Perplexity: 18.9120\n",
      "Epoch [3/3], Step [25400/25883], Loss: (2.3296, 2.2552, 0.0744), Perplexity: 10.2733\n",
      "Epoch [3/3], Step [25500/25883], Loss: (2.1751, 2.0772, 0.0978), Perplexity: 8.80270\n",
      "Epoch [3/3], Step [25600/25883], Loss: (2.3059, 2.1657, 0.1402), Perplexity: 10.0331\n",
      "Epoch [3/3], Step [25700/25883], Loss: (2.3038, 2.0252, 0.2785), Perplexity: 10.0119\n",
      "Epoch [3/3], Step [25800/25883], Loss: (2.6479, 2.4281, 0.2198), Perplexity: 14.1239\n",
      "Epoch [3/3], Step [25883/25883], Loss: (2.2536, 2.1428, 0.1108), Perplexity: 9.52208"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "generator.train()\n",
    "discriminator.train()\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        generator.zero_grad()\n",
    "        discriminator.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        out, img_feats = generator(images, captions)\n",
    "        \n",
    "        rho = 0.25\n",
    "        if random.random() < rho:\n",
    "            gen_captions = nn.functional.softmax(out)\n",
    "        else:\n",
    "            gen_captions = nn.functional.gumbel_softmax(out, hard=True)\n",
    "                \n",
    "        cap_feats = discriminator(img_feats, gen_captions).to(device)\n",
    "        \n",
    "        max_loss_1 = 0\n",
    "        max_loss_2 = 0\n",
    "        pos_pair_term = cosineSim(cap_feats[0], img_feats[0])\n",
    "        for i in range(1,batch_size):\n",
    "            max_loss_1 += max(0, 1 - pos_pair_term + cosineSim(cap_feats[0], img_feats[i]))\n",
    "            max_loss_2 += max(0, 1 - pos_pair_term + cosineSim(cap_feats[i], img_feats[0]))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss_A = criterion_A(out.view(-1, vocab_size), captions.view(-1))\n",
    "        loss_B = (max_loss_1 + max_loss_2)/(batch_size-1)\n",
    "        \n",
    "        B_weight = 0.5\n",
    "        loss = loss_A + B_weight*loss_B\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: (%.4f, %.4f, %.4f), Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), loss_A.item(), B_weight*loss_B.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "        # Save the weights.\n",
    "        if i_step % save_every == 0:\n",
    "            torch.save(generator.state_dict(), os.path.join('/media/test/UNTITLED/scratch', f'coop-generator-{epoch}-{i_step}.pkl'))\n",
    "            torch.save(discriminator.state_dict(), os.path.join('/media/test/UNTITLED/scratch', f'coop-discriminator-{epoch}-{i_step}.pkl'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
